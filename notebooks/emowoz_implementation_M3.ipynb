{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "# M3 RoBERTa + GRU Implementation\n",
        "## One-Turn-Ahead Frustration Prediction\n",
        "\n",
        "**Goal**: Beat M2's Macro-F1 (0.7396) while achieving production latency (‚â§15ms)\n",
        "\n",
        "**Architecture**: RoBERTa embeddings ‚Üí GRU temporal modeling ‚Üí Classification\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using device: cuda\n"
          ]
        }
      ],
      "source": [
        "# Setup and imports\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from transformers import RobertaModel, RobertaTokenizer\n",
        "import json\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.metrics import classification_report, f1_score, roc_auc_score\n",
        "from tqdm import tqdm\n",
        "import time\n",
        "import os\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Check device\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f\"Using device: {device}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "M3 Configuration:\n",
            "  model_name: roberta-base\n",
            "  max_length: 512\n",
            "  context_window: 3\n",
            "  gru_hidden_size: 128\n",
            "  gru_num_layers: 1\n",
            "  dropout: 0.1\n",
            "  batch_size: 16\n",
            "  learning_rate: 2e-05\n",
            "  epochs: 3\n",
            "  weight_decay: 0.01\n",
            "  class_weight_ratio: 13.7\n",
            "  patience: 2\n"
          ]
        }
      ],
      "source": [
        "# Model Configuration\n",
        "CONFIG = {\n",
        "    'model_name': 'roberta-base',\n",
        "    'max_length': 512,\n",
        "    'context_window': 3,\n",
        "    'gru_hidden_size': 128,\n",
        "    'gru_num_layers': 1,\n",
        "    'dropout': 0.1,\n",
        "    'batch_size': 16,\n",
        "    'learning_rate': 2e-5,\n",
        "    'epochs': 3,\n",
        "    'weight_decay': 0.01,\n",
        "    'class_weight_ratio': 13.7,\n",
        "    'patience': 2\n",
        "}\n",
        "\n",
        "print(\"M3 Configuration:\")\n",
        "for k, v in CONFIG.items():\n",
        "    print(f\"  {k}: {v}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "M3 RoBERTa + GRU model architecture defined\n"
          ]
        }
      ],
      "source": [
        "# M3 RoBERTa + GRU Model Architecture\n",
        "class RobertaGRU(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super(RobertaGRU, self).__init__()\n",
        "        self.config = config\n",
        "        \n",
        "        # RoBERTa for turn embeddings\n",
        "        self.roberta = RobertaModel.from_pretrained(config['model_name'])\n",
        "        self.roberta_hidden_size = self.roberta.config.hidden_size\n",
        "        \n",
        "        # GRU for temporal modeling\n",
        "        self.gru = nn.GRU(\n",
        "            input_size=self.roberta_hidden_size,\n",
        "            hidden_size=config['gru_hidden_size'],\n",
        "            num_layers=config['gru_num_layers'],\n",
        "            batch_first=True,\n",
        "            dropout=config['dropout'] if config['gru_num_layers'] > 1 else 0\n",
        "        )\n",
        "        \n",
        "        # Classification head\n",
        "        self.dropout = nn.Dropout(config['dropout'])\n",
        "        self.classifier = nn.Linear(config['gru_hidden_size'], 1)\n",
        "        \n",
        "    def forward(self, input_ids, attention_mask):\n",
        "        batch_size, seq_len, max_len = input_ids.shape\n",
        "        \n",
        "        # Reshape for RoBERTa processing\n",
        "        input_ids = input_ids.view(-1, max_len)\n",
        "        attention_mask = attention_mask.view(-1, max_len)\n",
        "        \n",
        "        # Get RoBERTa embeddings for each turn\n",
        "        roberta_outputs = self.roberta(input_ids=input_ids, attention_mask=attention_mask)\n",
        "        \n",
        "        # Use pooler output (CLS token representation)\n",
        "        turn_embeddings = roberta_outputs.pooler_output\n",
        "        \n",
        "        # Reshape back to sequence format\n",
        "        turn_embeddings = turn_embeddings.view(batch_size, seq_len, self.roberta_hidden_size)\n",
        "        \n",
        "        # Process through GRU\n",
        "        gru_output, _ = self.gru(turn_embeddings)\n",
        "        \n",
        "        # Use final hidden state\n",
        "        final_hidden = gru_output[:, -1, :]\n",
        "        \n",
        "        # Classification\n",
        "        output = self.dropout(final_hidden)\n",
        "        logits = self.classifier(output)\n",
        "        \n",
        "        return logits\n",
        "\n",
        "print(\"M3 RoBERTa + GRU model architecture defined\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Temporal dataset class defined\n"
          ]
        }
      ],
      "source": [
        "# Dataset class for M3 temporal data\n",
        "class EmoWOZTemporalDataset(Dataset):\n",
        "    def __init__(self, data_path, tokenizer, config):\n",
        "        self.tokenizer = tokenizer\n",
        "        self.max_length = config['max_length']\n",
        "        self.context_window = config['context_window']\n",
        "        \n",
        "        # Load data with error handling for corrupted lines\n",
        "        self.data = []\n",
        "        skipped_lines = 0\n",
        "        \n",
        "        with open(data_path, 'r') as f:\n",
        "            for line_num, line in enumerate(f, 1):\n",
        "                line = line.strip()\n",
        "                if not line:  # Skip empty lines\n",
        "                    continue\n",
        "                try:\n",
        "                    data_item = json.loads(line)\n",
        "                    self.data.append(data_item)\n",
        "                except json.JSONDecodeError as e:\n",
        "                    print(f\"Warning: Skipping invalid JSON at line {line_num} in {data_path}: {e}\")\n",
        "                    skipped_lines += 1\n",
        "                    continue\n",
        "        \n",
        "        print(f\"Loaded {len(self.data)} samples from {data_path}\")\n",
        "        if skipped_lines > 0:\n",
        "            print(f\"Skipped {skipped_lines} invalid lines\")\n",
        "        \n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "    \n",
        "    def parse_context_string(self, context_str):\n",
        "        \"\"\"Parse context string into individual turns\"\"\"\n",
        "        import re\n",
        "        \n",
        "        # Split by USER and SYSTEM tags\n",
        "        turns = []\n",
        "        \n",
        "        # Find all [USER] and [SYSTEM] tags and their positions\n",
        "        pattern = r'\\[(USER|SYSTEM)\\]'\n",
        "        matches = list(re.finditer(pattern, context_str))\n",
        "        \n",
        "        for i, match in enumerate(matches):\n",
        "            speaker = match.group(1)\n",
        "            start_pos = match.end()\n",
        "            \n",
        "            # Find the end position (start of next tag or end of string)\n",
        "            if i + 1 < len(matches):\n",
        "                end_pos = matches[i + 1].start()\n",
        "                text = context_str[start_pos:end_pos].strip()\n",
        "            else:\n",
        "                text = context_str[start_pos:].strip()\n",
        "            \n",
        "            if text:  # Only add non-empty turns\n",
        "                turns.append({\n",
        "                    'speaker': speaker,\n",
        "                    'text': text\n",
        "                })\n",
        "        \n",
        "        return turns\n",
        "    \n",
        "    def __getitem__(self, idx):\n",
        "        item = self.data[idx]\n",
        "        \n",
        "        # Parse context string into turns\n",
        "        context_str = item['context']\n",
        "        context_turns = self.parse_context_string(context_str)\n",
        "        \n",
        "        # Limit to context window (take last N turns)\n",
        "        if len(context_turns) > self.context_window:\n",
        "            context_turns = context_turns[-self.context_window:]\n",
        "        \n",
        "        # Tokenize each turn separately\n",
        "        turn_tokens = []\n",
        "        turn_masks = []\n",
        "        \n",
        "        for turn in context_turns:\n",
        "            text = turn['text']\n",
        "            encoded = self.tokenizer(\n",
        "                text,\n",
        "                max_length=self.max_length,\n",
        "                padding='max_length',\n",
        "                truncation=True,\n",
        "                return_tensors='pt'\n",
        "            )\n",
        "            \n",
        "            turn_tokens.append(encoded['input_ids'].squeeze(0))\n",
        "            turn_masks.append(encoded['attention_mask'].squeeze(0))\n",
        "        \n",
        "        # Pad sequence if needed\n",
        "        while len(turn_tokens) < self.context_window:\n",
        "            # Add padding turn\n",
        "            padding_tokens = torch.zeros(self.max_length, dtype=torch.long)\n",
        "            padding_mask = torch.zeros(self.max_length, dtype=torch.long)\n",
        "            turn_tokens.append(padding_tokens)\n",
        "            turn_masks.append(padding_mask)\n",
        "        \n",
        "        # Stack into tensors\n",
        "        input_ids = torch.stack(turn_tokens)\n",
        "        attention_mask = torch.stack(turn_masks)\n",
        "        \n",
        "        # Label\n",
        "        label = torch.tensor(item['label'], dtype=torch.float)\n",
        "        \n",
        "        return {\n",
        "            'input_ids': input_ids,\n",
        "            'attention_mask': attention_mask,\n",
        "            'label': label\n",
        "        }\n",
        "\n",
        "print(\"Temporal dataset class defined\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Warning: Skipping invalid JSON at line 1 in ../data/train.jsonl: Expecting value: line 1 column 1 (char 0)\n",
            "Loaded 25738 samples from ../data/train.jsonl\n",
            "Skipped 1 invalid lines\n",
            "Loaded 7409 samples from ../data/val.jsonl\n",
            "Loaded 7534 samples from ../data/test.jsonl\n",
            "Train batches: 1609\n",
            "Validation batches: 464\n",
            "Test batches: 471\n"
          ]
        }
      ],
      "source": [
        "# Load tokenizer and create datasets\n",
        "tokenizer = RobertaTokenizer.from_pretrained(CONFIG['model_name'])\n",
        "\n",
        "# Create datasets\n",
        "train_dataset = EmoWOZTemporalDataset('../data/train.jsonl', tokenizer, CONFIG)\n",
        "val_dataset = EmoWOZTemporalDataset('../data/val.jsonl', tokenizer, CONFIG)\n",
        "test_dataset = EmoWOZTemporalDataset('../data/test.jsonl', tokenizer, CONFIG)\n",
        "\n",
        "# Create data loaders\n",
        "train_loader = DataLoader(train_dataset, batch_size=CONFIG['batch_size'], shuffle=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=CONFIG['batch_size'], shuffle=False)\n",
        "test_loader = DataLoader(test_dataset, batch_size=CONFIG['batch_size'], shuffle=False)\n",
        "\n",
        "print(f\"Train batches: {len(train_loader)}\")\n",
        "print(f\"Validation batches: {len(val_loader)}\")\n",
        "print(f\"Test batches: {len(test_loader)}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of the model checkpoint at roberta-base were not used when initializing RobertaModel: ['lm_head.dense.weight', 'lm_head.layer_norm.weight', 'lm_head.layer_norm.bias', 'lm_head.bias', 'lm_head.dense.bias']\n",
            "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Total parameters: 124,990,593\n",
            "Trainable parameters: 124,990,593\n",
            "Model size: 476.8 MB\n"
          ]
        }
      ],
      "source": [
        "# Initialize model and training components\n",
        "model = RobertaGRU(CONFIG).to(device)\n",
        "\n",
        "# Loss function with class weights\n",
        "pos_weight = torch.tensor(CONFIG['class_weight_ratio']).to(device)\n",
        "criterion = nn.BCEWithLogitsLoss(pos_weight=pos_weight)\n",
        "\n",
        "# Optimizer\n",
        "optimizer = optim.AdamW(model.parameters(), lr=CONFIG['learning_rate'], weight_decay=CONFIG['weight_decay'])\n",
        "\n",
        "# Model info\n",
        "total_params = sum(p.numel() for p in model.parameters())\n",
        "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "\n",
        "print(f\"Total parameters: {total_params:,}\")\n",
        "print(f\"Trainable parameters: {trainable_params:,}\")\n",
        "print(f\"Model size: {total_params * 4 / 1024**2:.1f} MB\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training and evaluation functions defined\n"
          ]
        }
      ],
      "source": [
        "# Training and evaluation functions\n",
        "def train_epoch(model, train_loader, criterion, optimizer, device):\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "    \n",
        "    for batch in tqdm(train_loader, desc=\"Training\"):\n",
        "        input_ids = batch['input_ids'].to(device)\n",
        "        attention_mask = batch['attention_mask'].to(device)\n",
        "        labels = batch['label'].to(device)\n",
        "        \n",
        "        optimizer.zero_grad()\n",
        "        \n",
        "        outputs = model(input_ids, attention_mask)\n",
        "        # Squeeze only the last dimension, preserve batch dimension\n",
        "        outputs = outputs.squeeze(-1)\n",
        "        loss = criterion(outputs, labels)\n",
        "        \n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        \n",
        "        total_loss += loss.item()\n",
        "    \n",
        "    return total_loss / len(train_loader)\n",
        "\n",
        "def evaluate(model, eval_loader, criterion, device):\n",
        "    model.eval()\n",
        "    total_loss = 0\n",
        "    all_preds = []\n",
        "    all_labels = []\n",
        "    \n",
        "    with torch.no_grad():\n",
        "        for batch in tqdm(eval_loader, desc=\"Evaluating\"):\n",
        "            input_ids = batch['input_ids'].to(device)\n",
        "            attention_mask = batch['attention_mask'].to(device)\n",
        "            labels = batch['label'].to(device)\n",
        "            \n",
        "            outputs = model(input_ids, attention_mask)\n",
        "            # Squeeze only the last dimension, preserve batch dimension\n",
        "            outputs = outputs.squeeze(-1)\n",
        "            loss = criterion(outputs, labels)\n",
        "            \n",
        "            total_loss += loss.item()\n",
        "            \n",
        "            # Get predictions\n",
        "            preds = torch.sigmoid(outputs).cpu().numpy()\n",
        "            all_preds.extend(preds)\n",
        "            all_labels.extend(labels.cpu().numpy())\n",
        "    \n",
        "    # Calculate metrics\n",
        "    avg_loss = total_loss / len(eval_loader)\n",
        "    preds_binary = (np.array(all_preds) > 0.5).astype(int)\n",
        "    \n",
        "    macro_f1 = f1_score(all_labels, preds_binary, average='macro')\n",
        "    auc = roc_auc_score(all_labels, all_preds)\n",
        "    \n",
        "    return avg_loss, macro_f1, auc, np.array(all_labels), np.array(all_preds), preds_binary\n",
        "\n",
        "print(\"Training and evaluation functions defined\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Starting M3 training...\n",
            "==================================================\n",
            "\n",
            "Epoch 1/3\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1609/1609 [15:31<00:00,  1.73it/s]\n",
            "Evaluating: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 464/464 [01:33<00:00,  4.97it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train Loss: 0.8781\n",
            "Val Loss: 0.7599\n",
            "Val Macro-F1: 0.7180\n",
            "Val AUC: 0.8837\n",
            "‚úÖ New best model saved! Macro-F1: 0.7180\n",
            "\n",
            "Epoch 2/3\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1609/1609 [15:32<00:00,  1.73it/s]\n",
            "Evaluating: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 464/464 [01:33<00:00,  4.98it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train Loss: 0.7995\n",
            "Val Loss: 0.7650\n",
            "Val Macro-F1: 0.7212\n",
            "Val AUC: 0.8858\n",
            "‚úÖ New best model saved! Macro-F1: 0.7212\n",
            "\n",
            "Epoch 3/3\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1609/1609 [15:32<00:00,  1.73it/s]\n",
            "Evaluating: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 464/464 [01:33<00:00,  4.97it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train Loss: 0.7968\n",
            "Val Loss: 0.7132\n",
            "Val Macro-F1: 0.7227\n",
            "Val AUC: 0.8882\n",
            "‚úÖ New best model saved! Macro-F1: 0.7227\n",
            "\n",
            "üéâ Training completed in 3078.0 seconds (51.3 minutes)\n",
            "Best validation Macro-F1: 0.7227\n"
          ]
        }
      ],
      "source": [
        "# Training loop\n",
        "print(\"Starting M3 training...\")\n",
        "print(\"=\"*50)\n",
        "\n",
        "best_macro_f1 = 0\n",
        "patience_counter = 0\n",
        "training_history = []\n",
        "\n",
        "start_time = time.time()\n",
        "\n",
        "for epoch in range(CONFIG['epochs']):\n",
        "    print(f\"\\nEpoch {epoch + 1}/{CONFIG['epochs']}\")\n",
        "    \n",
        "    # Training\n",
        "    train_loss = train_epoch(model, train_loader, criterion, optimizer, device)\n",
        "    \n",
        "    # Validation\n",
        "    val_loss, val_macro_f1, val_auc, _, _, _ = evaluate(model, val_loader, criterion, device)\n",
        "    \n",
        "    # Log results\n",
        "    print(f\"Train Loss: {train_loss:.4f}\")\n",
        "    print(f\"Val Loss: {val_loss:.4f}\")\n",
        "    print(f\"Val Macro-F1: {val_macro_f1:.4f}\")\n",
        "    print(f\"Val AUC: {val_auc:.4f}\")\n",
        "    \n",
        "    # Save training history\n",
        "    training_history.append({\n",
        "        'epoch': epoch + 1,\n",
        "        'train_loss': train_loss,\n",
        "        'val_loss': val_loss,\n",
        "        'val_macro_f1': val_macro_f1,\n",
        "        'val_auc': val_auc\n",
        "    })\n",
        "    \n",
        "    # Early stopping\n",
        "    if val_macro_f1 > best_macro_f1:\n",
        "        best_macro_f1 = val_macro_f1\n",
        "        patience_counter = 0\n",
        "        # Save best model\n",
        "        os.makedirs('../checkpoints/M3_roberta_gru', exist_ok=True)\n",
        "        torch.save(model.state_dict(), '../checkpoints/M3_roberta_gru/best_model.pt')\n",
        "        print(f\"‚úÖ New best model saved! Macro-F1: {best_macro_f1:.4f}\")\n",
        "    else:\n",
        "        patience_counter += 1\n",
        "        print(f\"‚è∏Ô∏è No improvement. Patience: {patience_counter}/{CONFIG['patience']}\")\n",
        "    \n",
        "    if patience_counter >= CONFIG['patience']:\n",
        "        print(f\"Early stopping triggered after {epoch + 1} epochs\")\n",
        "        break\n",
        "\n",
        "training_time = time.time() - start_time\n",
        "print(f\"\\nüéâ Training completed in {training_time:.1f} seconds ({training_time/60:.1f} minutes)\")\n",
        "print(f\"Best validation Macro-F1: {best_macro_f1:.4f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚úÖ Best model loaded for final evaluation\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Evaluating: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 471/471 [01:34<00:00,  4.96it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "üìä FINAL M3 TEST RESULTS\n",
            "========================================\n",
            "Test Loss: 0.8602\n",
            "Test Macro-F1: 0.7408\n",
            "Test AUC: 0.8768\n",
            "\n",
            "Detailed Classification Report:\n",
            "                    precision    recall  f1-score   support\n",
            "\n",
            "    Not Frustrated     0.9807    0.9007    0.9390      6930\n",
            "Will Be Frustrated     0.4115    0.7964    0.5426       604\n",
            "\n",
            "          accuracy                         0.8924      7534\n",
            "         macro avg     0.6961    0.8485    0.7408      7534\n",
            "      weighted avg     0.9350    0.8924    0.9072      7534\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "# Load best model and test evaluation\n",
        "model.load_state_dict(torch.load('../checkpoints/M3_roberta_gru/best_model.pt', weights_only=True))\n",
        "print(\"‚úÖ Best model loaded for final evaluation\")\n",
        "\n",
        "# Final test evaluation\n",
        "test_loss, test_macro_f1, test_auc, test_labels, test_probs, test_preds = evaluate(model, test_loader, criterion, device)\n",
        "\n",
        "print(\"\\nüìä FINAL M3 TEST RESULTS\")\n",
        "print(\"=\"*40)\n",
        "print(f\"Test Loss: {test_loss:.4f}\")\n",
        "print(f\"Test Macro-F1: {test_macro_f1:.4f}\")\n",
        "print(f\"Test AUC: {test_auc:.4f}\")\n",
        "\n",
        "# Detailed classification report\n",
        "print(\"\\nDetailed Classification Report:\")\n",
        "target_names = ['Not Frustrated', 'Will Be Frustrated']\n",
        "print(classification_report(test_labels, test_preds, target_names=target_names, digits=4))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "‚ö° LATENCY BENCHMARKING\n",
            "==============================\n",
            "Warm-up completed, measuring latency...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Latency test: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 471/471 [01:55<00:00,  4.08it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Average Latency: 11.57ms\n",
            "Median Latency: 15.17ms\n",
            "95th Percentile: 15.42ms\n",
            "99th Percentile: 15.45ms\n",
            "Throughput: 86.5 samples/sec\n",
            "‚úÖ LATENCY TARGET MET: 11.57ms ‚â§ 15.0ms\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "# Latency benchmarking\n",
        "print(\"\\n‚ö° LATENCY BENCHMARKING\")\n",
        "print(\"=\"*30)\n",
        "\n",
        "model.eval()\n",
        "latencies = []\n",
        "\n",
        "# Warm-up\n",
        "for i, batch in enumerate(test_loader):\n",
        "    if i >= 5:  # Warm-up with 5 batches\n",
        "        break\n",
        "    input_ids = batch['input_ids'].to(device)\n",
        "    attention_mask = batch['attention_mask'].to(device)\n",
        "    with torch.no_grad():\n",
        "        _ = model(input_ids, attention_mask)\n",
        "\n",
        "print(\"Warm-up completed, measuring latency...\")\n",
        "\n",
        "# Measure latency\n",
        "with torch.no_grad():\n",
        "    for batch in tqdm(test_loader, desc=\"Latency test\"):\n",
        "        input_ids = batch['input_ids'].to(device)\n",
        "        attention_mask = batch['attention_mask'].to(device)\n",
        "        \n",
        "        for i in range(input_ids.shape[0]):  # Process each sample individually\n",
        "            single_input = input_ids[i:i+1]\n",
        "            single_mask = attention_mask[i:i+1]\n",
        "            \n",
        "            start_time = time.perf_counter()\n",
        "            _ = model(single_input, single_mask)\n",
        "            end_time = time.perf_counter()\n",
        "            \n",
        "            latencies.append((end_time - start_time) * 1000)  # Convert to milliseconds\n",
        "\n",
        "# Calculate latency statistics\n",
        "latencies = np.array(latencies)\n",
        "avg_latency = np.mean(latencies)\n",
        "median_latency = np.median(latencies)\n",
        "p95_latency = np.percentile(latencies, 95)\n",
        "p99_latency = np.percentile(latencies, 99)\n",
        "\n",
        "print(f\"Average Latency: {avg_latency:.2f}ms\")\n",
        "print(f\"Median Latency: {median_latency:.2f}ms\")\n",
        "print(f\"95th Percentile: {p95_latency:.2f}ms\")\n",
        "print(f\"99th Percentile: {p99_latency:.2f}ms\")\n",
        "print(f\"Throughput: {1000/avg_latency:.1f} samples/sec\")\n",
        "\n",
        "# Check latency target\n",
        "latency_target = 15.0  # ms\n",
        "if avg_latency <= latency_target:\n",
        "    print(f\"‚úÖ LATENCY TARGET MET: {avg_latency:.2f}ms ‚â§ {latency_target}ms\")\n",
        "else:\n",
        "    print(f\"‚ùå LATENCY TARGET MISSED: {avg_latency:.2f}ms > {latency_target}ms\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "üìà MODEL COMPARISON\n",
            "==================================================\n",
            "         Model  Macro-F1  Latency (ms)  Accuracy\n",
            "   M1 BERT-CLS    0.7156       10.0700    0.9158\n",
            "M2 RoBERTa-CLS    0.7396       72.3900    0.8912\n",
            "M3 RoBERTa-GRU    0.7408       11.5653    0.8924\n",
            "\n",
            "üèÜ M3 BEATS M2! Improvement: +0.0012 Macro-F1 (0.2%)\n",
            "\n",
            "üöÄ PRODUCTION READINESS ASSESSMENT\n",
            "========================================\n",
            "Macro-F1 Target (‚â•0.3): 0.7408 ‚úÖ PASS\n",
            "Latency Target (‚â§15.0ms): 11.57ms ‚úÖ PASS\n",
            "\n",
            "üéâ M3 IS PRODUCTION READY!\n"
          ]
        }
      ],
      "source": [
        "# Performance comparison with M1 and M2\n",
        "print(\"\\nüìà MODEL COMPARISON\")\n",
        "print(\"=\"*50)\n",
        "\n",
        "# M1 and M2 results (from previous reports)\n",
        "m1_results = {\n",
        "    'macro_f1': 0.7156,\n",
        "    'latency': 10.07,\n",
        "    'accuracy': 0.9158\n",
        "}\n",
        "\n",
        "m2_results = {\n",
        "    'macro_f1': 0.7396,\n",
        "    'latency': 72.39,\n",
        "    'accuracy': 0.8912\n",
        "}\n",
        "\n",
        "m3_results = {\n",
        "    'macro_f1': test_macro_f1,\n",
        "    'latency': avg_latency,\n",
        "    'accuracy': np.mean(test_labels == test_preds)\n",
        "}\n",
        "\n",
        "# Create comparison table\n",
        "comparison_df = pd.DataFrame({\n",
        "    'Model': ['M1 BERT-CLS', 'M2 RoBERTa-CLS', 'M3 RoBERTa-GRU'],\n",
        "    'Macro-F1': [m1_results['macro_f1'], m2_results['macro_f1'], m3_results['macro_f1']],\n",
        "    'Latency (ms)': [m1_results['latency'], m2_results['latency'], m3_results['latency']],\n",
        "    'Accuracy': [m1_results['accuracy'], m2_results['accuracy'], m3_results['accuracy']]\n",
        "})\n",
        "\n",
        "print(comparison_df.to_string(index=False, float_format='%.4f'))\n",
        "\n",
        "# Check if M3 beats M2\n",
        "if m3_results['macro_f1'] > m2_results['macro_f1']:\n",
        "    improvement = m3_results['macro_f1'] - m2_results['macro_f1']\n",
        "    print(f\"\\nüèÜ M3 BEATS M2! Improvement: +{improvement:.4f} Macro-F1 ({improvement/m2_results['macro_f1']*100:.1f}%)\")\n",
        "else:\n",
        "    decline = m2_results['macro_f1'] - m3_results['macro_f1']\n",
        "    print(f\"\\nüìâ M3 underperforms M2: -{decline:.4f} Macro-F1 ({decline/m2_results['macro_f1']*100:.1f}%)\")\n",
        "\n",
        "# Production readiness assessment\n",
        "print(\"\\nüöÄ PRODUCTION READINESS ASSESSMENT\")\n",
        "print(\"=\"*40)\n",
        "target_f1 = 0.30\n",
        "target_latency = 15.0\n",
        "\n",
        "f1_status = \"‚úÖ PASS\" if m3_results['macro_f1'] >= target_f1 else \"‚ùå FAIL\"\n",
        "latency_status = \"‚úÖ PASS\" if m3_results['latency'] <= target_latency else \"‚ùå FAIL\"\n",
        "\n",
        "print(f\"Macro-F1 Target (‚â•{target_f1}): {m3_results['macro_f1']:.4f} {f1_status}\")\n",
        "print(f\"Latency Target (‚â§{target_latency}ms): {m3_results['latency']:.2f}ms {latency_status}\")\n",
        "\n",
        "if m3_results['macro_f1'] >= target_f1 and m3_results['latency'] <= target_latency:\n",
        "    print(\"\\nüéâ M3 IS PRODUCTION READY!\")\n",
        "else:\n",
        "    print(\"\\n‚ö†Ô∏è M3 needs optimization for production deployment\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚úÖ M3 results saved to ../results/M3_roberta_gru_results.json\n",
            "‚úÖ M3 model checkpoint saved to ../checkpoints/M3_roberta_gru/best_model.pt\n",
            "\n",
            "üéØ M3 IMPLEMENTATION COMPLETE!\n"
          ]
        }
      ],
      "source": [
        "# Save M3 results\n",
        "os.makedirs('../results', exist_ok=True)\n",
        "\n",
        "# Save detailed results\n",
        "m3_detailed_results = {\n",
        "    'model_name': 'M3_RoBERTa_GRU',\n",
        "    'config': CONFIG,\n",
        "    'training_history': training_history,\n",
        "    'test_results': {\n",
        "        'macro_f1': float(test_macro_f1),\n",
        "        'auc': float(test_auc),\n",
        "        'accuracy': float(np.mean(test_labels == test_preds)),\n",
        "        'test_loss': float(test_loss)\n",
        "    },\n",
        "    'latency_results': {\n",
        "        'avg_latency_ms': float(avg_latency),\n",
        "        'median_latency_ms': float(median_latency),\n",
        "        'p95_latency_ms': float(p95_latency),\n",
        "        'p99_latency_ms': float(p99_latency),\n",
        "        'throughput_samples_per_sec': float(1000/avg_latency)\n",
        "    },\n",
        "    'model_comparison': {\n",
        "        'M1_BERT_CLS': m1_results,\n",
        "        'M2_RoBERTa_CLS': m2_results,\n",
        "        'M3_RoBERTa_GRU': m3_results\n",
        "    },\n",
        "    'training_time_seconds': training_time\n",
        "}\n",
        "\n",
        "with open('../results/M3_roberta_gru_results.json', 'w') as f:\n",
        "    json.dump(m3_detailed_results, f, indent=2)\n",
        "\n",
        "print(\"‚úÖ M3 results saved to ../results/M3_roberta_gru_results.json\")\n",
        "print(\"‚úÖ M3 model checkpoint saved to ../checkpoints/M3_roberta_gru/best_model.pt\")\n",
        "print(\"\\nüéØ M3 IMPLEMENTATION COMPLETE!\")\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
