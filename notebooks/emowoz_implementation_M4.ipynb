{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "# 🚀 EmoWOZ Final Implementation: Days 5-7\n",
        "## M4 DialoGPT + Comprehensive Evaluation + Final Package\n",
        "\n",
        "**Project**: One-Turn-Ahead Frustration Forecasting in Task-Oriented Dialogs  \n",
        "**Current Status**: M3 RoBERTa-GRU BREAKTHROUGH (Macro-F1: 0.7408, Latency: 11.57ms) ✅  \n",
        "**Next Goals**: \n",
        "- **Day 5**: M4 DialoGPT (target: beat M3's 0.7408)\n",
        "- **Day 6**: Cross-model evaluation and benchmarking\n",
        "- **Day 7**: Final documentation and package\n",
        "\n",
        "---\n",
        "\n",
        "## 📋 Implementation Plan\n",
        "\n",
        "### **Day 5: M4 DialoGPT Fine-tuned Model**\n",
        "- DialoGPT-small fine-tuning\n",
        "- Longer context (5 turns, max_length=1024)\n",
        "- Last token representation\n",
        "- Target: Macro-F1 > 0.7408\n",
        "\n",
        "### **Day 6: Comprehensive Evaluation**\n",
        "- eval.py script for all models (M1-M4)\n",
        "- Statistical significance testing\n",
        "- Cross-model latency benchmarking\n",
        "- Error analysis\n",
        "\n",
        "### **Day 7: Final Package**\n",
        "- Complete documentation\n",
        "- Model comparison report\n",
        "- Reproducibility guide\n",
        "- Benchmark package\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using device: cuda\n",
            "CUDA available: True\n",
            "GPU: NVIDIA H100 PCIe\n",
            "GPU Memory: 79.1 GB\n"
          ]
        }
      ],
      "source": [
        "# Setup and imports for M4-M7 implementation\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from transformers import GPT2LMHeadModel, GPT2Tokenizer, RobertaModel, RobertaTokenizer, BertModel, BertTokenizer\n",
        "import json\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.metrics import classification_report, f1_score, roc_auc_score, accuracy_score\n",
        "from scipy import stats\n",
        "from tqdm import tqdm\n",
        "import time\n",
        "import os\n",
        "import warnings\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Check device\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f\"Using device: {device}\")\n",
        "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
        "if torch.cuda.is_available():\n",
        "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
        "    print(f\"GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.1f} GB\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "# 📊 DAY 5: M4 DialoGPT Implementation\n",
        "\n",
        "## 🎯 M4 Objectives\n",
        "- **Target**: Beat M3's Macro-F1 (0.7408)\n",
        "- **Innovation**: Longer context (5 turns vs 3 turns)\n",
        "- **Architecture**: DialoGPT-small + classification head\n",
        "- **Context**: max_length=1024 (vs M3's 512)\n",
        "- **Representation**: Last token (vs pooler output)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "M4 DialoGPT Configuration:\n",
            "  model_name: microsoft/DialoGPT-small\n",
            "  max_length: 1024\n",
            "  context_window: 5\n",
            "  dropout: 0.1\n",
            "  batch_size: 8\n",
            "  learning_rate: 1e-05\n",
            "  epochs: 5\n",
            "  weight_decay: 0.01\n",
            "  class_weight_ratio: 13.7\n",
            "  patience: 3\n",
            "\n",
            "📊 M4 vs M3 Differences:\n",
            "  Context Window: 5 vs 3 turns\n",
            "  Max Length: 1024 vs 512 tokens\n",
            "  Batch Size: 8 vs 16 (memory optimization)\n",
            "  Learning Rate: 1e-05 vs 2e-5\n"
          ]
        }
      ],
      "source": [
        "# M4 DialoGPT Configuration\n",
        "M4_CONFIG = {\n",
        "    'model_name': 'microsoft/DialoGPT-small',\n",
        "    'max_length': 1024,       # Longer context than M3 (512)\n",
        "    'context_window': 5,      # More turns than M3 (3)\n",
        "    'dropout': 0.1,\n",
        "    'batch_size': 8,          # Smaller due to longer sequences\n",
        "    'learning_rate': 1e-5,    # Lower LR for fine-tuning\n",
        "    'epochs': 5,              # More epochs for convergence\n",
        "    'weight_decay': 0.01,\n",
        "    'class_weight_ratio': 13.7,\n",
        "    'patience': 3\n",
        "}\n",
        "\n",
        "print(\"M4 DialoGPT Configuration:\")\n",
        "for k, v in M4_CONFIG.items():\n",
        "    print(f\"  {k}: {v}\")\n",
        "\n",
        "# Compare with M3\n",
        "print(f\"\\n📊 M4 vs M3 Differences:\")\n",
        "print(f\"  Context Window: {M4_CONFIG['context_window']} vs 3 turns\")\n",
        "print(f\"  Max Length: {M4_CONFIG['max_length']} vs 512 tokens\")\n",
        "print(f\"  Batch Size: {M4_CONFIG['batch_size']} vs 16 (memory optimization)\")\n",
        "print(f\"  Learning Rate: {M4_CONFIG['learning_rate']} vs 2e-5\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "M4 DialoGPT Classifier architecture defined\n",
            "Key innovations:\n",
            "  - Uses DialoGPT transformer (dialogue-specific pre-training)\n",
            "  - Last token representation (captures full dialogue context)\n",
            "  - Longer context window (5 turns vs M3's 3)\n",
            "  - Larger sequence length (1024 vs M3's 512)\n"
          ]
        }
      ],
      "source": [
        "# M4 DialoGPT Model Architecture\n",
        "class DialoGPTClassifier(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super(DialoGPTClassifier, self).__init__()\n",
        "        self.config = config\n",
        "        \n",
        "        # DialoGPT for dialogue understanding\n",
        "        self.gpt = GPT2LMHeadModel.from_pretrained(config['model_name'])\n",
        "        self.hidden_size = self.gpt.config.hidden_size\n",
        "        \n",
        "        # Freeze language modeling head (we only need the transformer)\n",
        "        for param in self.gpt.lm_head.parameters():\n",
        "            param.requires_grad = False\n",
        "            \n",
        "        # Classification head\n",
        "        self.dropout = nn.Dropout(config['dropout'])\n",
        "        self.classifier = nn.Linear(self.hidden_size, 1)\n",
        "        \n",
        "    def forward(self, input_ids, attention_mask):\n",
        "        # Get DialoGPT transformer outputs\n",
        "        outputs = self.gpt.transformer(input_ids=input_ids, attention_mask=attention_mask)\n",
        "        \n",
        "        # Use last token representation (dialogue completion)\n",
        "        # Find the last non-padded token for each sequence\n",
        "        batch_size = input_ids.shape[0]\n",
        "        last_token_indices = attention_mask.sum(dim=1) - 1  # Last actual token\n",
        "        \n",
        "        # Extract last token representations\n",
        "        last_hidden_states = []\n",
        "        for i in range(batch_size):\n",
        "            last_idx = last_token_indices[i]\n",
        "            last_hidden_states.append(outputs.last_hidden_state[i, last_idx, :])\n",
        "        \n",
        "        last_hidden = torch.stack(last_hidden_states)\n",
        "        \n",
        "        # Classification\n",
        "        output = self.dropout(last_hidden)\n",
        "        logits = self.classifier(output)\n",
        "        \n",
        "        return logits\n",
        "\n",
        "print(\"M4 DialoGPT Classifier architecture defined\")\n",
        "print(\"Key innovations:\")\n",
        "print(\"  - Uses DialoGPT transformer (dialogue-specific pre-training)\")\n",
        "print(\"  - Last token representation (captures full dialogue context)\")\n",
        "print(\"  - Longer context window (5 turns vs M3's 3)\")\n",
        "print(\"  - Larger sequence length (1024 vs M3's 512)\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "M4 DialoGPT Dataset class defined\n",
            "Key features:\n",
            "  - Longer context window: 5 turns\n",
            "  - Larger max length: 1024 tokens\n",
            "  - Dialogue-style formatting for DialoGPT\n",
            "  - User/System speaker formatting\n"
          ]
        }
      ],
      "source": [
        "# M4 Dataset class for longer context\n",
        "class EmoWOZDialoGPTDataset(Dataset):\n",
        "    def __init__(self, data_path, tokenizer, config):\n",
        "        self.tokenizer = tokenizer\n",
        "        self.max_length = config['max_length']\n",
        "        self.context_window = config['context_window']\n",
        "        \n",
        "        # Load data with error handling\n",
        "        self.data = []\n",
        "        skipped_lines = 0\n",
        "        \n",
        "        with open(data_path, 'r') as f:\n",
        "            for line_num, line in enumerate(f, 1):\n",
        "                line = line.strip()\n",
        "                if not line:\n",
        "                    continue\n",
        "                try:\n",
        "                    data_item = json.loads(line)\n",
        "                    self.data.append(data_item)\n",
        "                except json.JSONDecodeError as e:\n",
        "                    skipped_lines += 1\n",
        "                    continue\n",
        "        \n",
        "        print(f\"M4 Dataset: Loaded {len(self.data)} samples from {data_path}\")\n",
        "        if skipped_lines > 0:\n",
        "            print(f\"Skipped {skipped_lines} invalid lines\")\n",
        "    \n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "    \n",
        "    def parse_context_string(self, context_str):\n",
        "        \"\"\"Parse context string into individual turns\"\"\"\n",
        "        import re\n",
        "        \n",
        "        turns = []\n",
        "        pattern = r'\\[(USER|SYSTEM)\\]'\n",
        "        matches = list(re.finditer(pattern, context_str))\n",
        "        \n",
        "        for i, match in enumerate(matches):\n",
        "            speaker = match.group(1)\n",
        "            start_pos = match.end()\n",
        "            \n",
        "            if i + 1 < len(matches):\n",
        "                end_pos = matches[i + 1].start()\n",
        "                text = context_str[start_pos:end_pos].strip()\n",
        "            else:\n",
        "                text = context_str[start_pos:].strip()\n",
        "            \n",
        "            if text:\n",
        "                turns.append({\n",
        "                    'speaker': speaker,\n",
        "                    'text': text\n",
        "                })\n",
        "        \n",
        "        return turns\n",
        "    \n",
        "    def __getitem__(self, idx):\n",
        "        item = self.data[idx]\n",
        "        \n",
        "        # Parse context string into turns\n",
        "        context_str = item['context']\n",
        "        context_turns = self.parse_context_string(context_str)\n",
        "        \n",
        "        # Take last N turns (longer context for M4)\n",
        "        if len(context_turns) > self.context_window:\n",
        "            context_turns = context_turns[-self.context_window:]\n",
        "        \n",
        "        # Create dialogue string for DialoGPT\n",
        "        dialogue_text = \"\"\n",
        "        for turn in context_turns:\n",
        "            if turn['speaker'] == 'USER':\n",
        "                dialogue_text += f\"User: {turn['text']} \"\n",
        "            else:\n",
        "                dialogue_text += f\"System: {turn['text']} \"\n",
        "        \n",
        "        # Add the current turn\n",
        "        current_text = item.get('text', '')\n",
        "        if current_text:\n",
        "            dialogue_text += f\"User: {current_text}\"\n",
        "        \n",
        "        # Tokenize the full dialogue\n",
        "        encoded = self.tokenizer(\n",
        "            dialogue_text.strip(),\n",
        "            max_length=self.max_length,\n",
        "            padding='max_length',\n",
        "            truncation=True,\n",
        "            return_tensors='pt'\n",
        "        )\n",
        "        \n",
        "        # Label\n",
        "        label = torch.tensor(item['label'], dtype=torch.float)\n",
        "        \n",
        "        return {\n",
        "            'input_ids': encoded['input_ids'].squeeze(0),\n",
        "            'attention_mask': encoded['attention_mask'].squeeze(0),\n",
        "            'label': label\n",
        "        }\n",
        "\n",
        "print(\"M4 DialoGPT Dataset class defined\")\n",
        "print(\"Key features:\")\n",
        "print(f\"  - Longer context window: {M4_CONFIG['context_window']} turns\")\n",
        "print(f\"  - Larger max length: {M4_CONFIG['max_length']} tokens\")\n",
        "print(\"  - Dialogue-style formatting for DialoGPT\")\n",
        "print(\"  - User/System speaker formatting\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Using pad_token, but it is not set yet.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loading DialoGPT tokenizer...\n",
            "Tokenizer loaded: microsoft/DialoGPT-small\n",
            "Vocab size: 50257\n",
            "Pad token: <|endoftext|>\n",
            "M4 Dataset: Loaded 25738 samples from ../data/train.jsonl\n",
            "Skipped 1 invalid lines\n",
            "M4 Dataset: Loaded 7409 samples from ../data/val.jsonl\n",
            "M4 Dataset: Loaded 7534 samples from ../data/test.jsonl\n",
            "\n",
            "📊 M4 Data Loading Complete:\n",
            "Train batches: 3218 (batch_size=8)\n",
            "Validation batches: 927\n",
            "Test batches: 942\n",
            "Total samples: 25738 train, 7409 val, 7534 test\n"
          ]
        }
      ],
      "source": [
        "# Step 2: M4 Data Loading & Training Setup\n",
        "\n",
        "# Load DialoGPT tokenizer\n",
        "print(\"Loading DialoGPT tokenizer...\")\n",
        "tokenizer = GPT2Tokenizer.from_pretrained(M4_CONFIG['model_name'])\n",
        "\n",
        "# Add padding token (DialoGPT doesn't have one by default)\n",
        "if tokenizer.pad_token is None:\n",
        "    tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "print(f\"Tokenizer loaded: {M4_CONFIG['model_name']}\")\n",
        "print(f\"Vocab size: {len(tokenizer)}\")\n",
        "print(f\"Pad token: {tokenizer.pad_token}\")\n",
        "\n",
        "# Create M4 datasets with longer context\n",
        "train_dataset = EmoWOZDialoGPTDataset('../data/train.jsonl', tokenizer, M4_CONFIG)\n",
        "val_dataset = EmoWOZDialoGPTDataset('../data/val.jsonl', tokenizer, M4_CONFIG)\n",
        "test_dataset = EmoWOZDialoGPTDataset('../data/test.jsonl', tokenizer, M4_CONFIG)\n",
        "\n",
        "# Create data loaders with smaller batch size\n",
        "train_loader = DataLoader(train_dataset, batch_size=M4_CONFIG['batch_size'], shuffle=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=M4_CONFIG['batch_size'], shuffle=False)\n",
        "test_loader = DataLoader(test_dataset, batch_size=M4_CONFIG['batch_size'], shuffle=False)\n",
        "\n",
        "print(f\"\\n📊 M4 Data Loading Complete:\")\n",
        "print(f\"Train batches: {len(train_loader)} (batch_size={M4_CONFIG['batch_size']})\")\n",
        "print(f\"Validation batches: {len(val_loader)}\")\n",
        "print(f\"Test batches: {len(test_loader)}\")\n",
        "print(f\"Total samples: {len(train_dataset)} train, {len(val_dataset)} val, {len(test_dataset)} test\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Initializing M4 DialoGPT model...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "🏗️ M4 Model Information:\n",
            "Total parameters: 124,440,577\n",
            "Trainable parameters: 85,843,201\n",
            "Frozen parameters: 38,597,376 (LM head)\n",
            "Model size: 474.7 MB\n",
            "Hidden size: 768\n",
            "\n",
            "📊 M4 vs M3 Model Comparison:\n",
            "M4 Parameters: 124,440,577\n",
            "M3 Parameters: ~125M (RoBERTa + GRU)\n",
            "M4 Context: 5 turns, 1024 tokens\n",
            "M3 Context: 3 turns, 512 tokens\n"
          ]
        }
      ],
      "source": [
        "# Initialize M4 Model and Training Components\n",
        "print(\"Initializing M4 DialoGPT model...\")\n",
        "\n",
        "# Initialize model\n",
        "model = DialoGPTClassifier(M4_CONFIG).to(device)\n",
        "\n",
        "# Loss function with class weights\n",
        "pos_weight = torch.tensor(M4_CONFIG['class_weight_ratio']).to(device)\n",
        "criterion = nn.BCEWithLogitsLoss(pos_weight=pos_weight)\n",
        "\n",
        "# Optimizer (lower learning rate for fine-tuning)\n",
        "optimizer = optim.AdamW(model.parameters(), lr=M4_CONFIG['learning_rate'], weight_decay=M4_CONFIG['weight_decay'])\n",
        "\n",
        "# Model information\n",
        "total_params = sum(p.numel() for p in model.parameters())\n",
        "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "frozen_params = total_params - trainable_params\n",
        "\n",
        "print(f\"\\n🏗️ M4 Model Information:\")\n",
        "print(f\"Total parameters: {total_params:,}\")\n",
        "print(f\"Trainable parameters: {trainable_params:,}\")\n",
        "print(f\"Frozen parameters: {frozen_params:,} (LM head)\")\n",
        "print(f\"Model size: {total_params * 4 / 1024**2:.1f} MB\")\n",
        "print(f\"Hidden size: {model.hidden_size}\")\n",
        "\n",
        "# Compare with M3\n",
        "print(f\"\\n📊 M4 vs M3 Model Comparison:\")\n",
        "print(f\"M4 Parameters: {total_params:,}\")\n",
        "print(f\"M3 Parameters: ~125M (RoBERTa + GRU)\")\n",
        "print(f\"M4 Context: {M4_CONFIG['context_window']} turns, {M4_CONFIG['max_length']} tokens\")\n",
        "print(f\"M3 Context: 3 turns, 512 tokens\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "M4 training and evaluation functions defined\n",
            "Key features:\n",
            "  - Optimized for DialoGPT longer sequences\n",
            "  - Handles last token representation\n",
            "  - Memory-efficient batch processing\n"
          ]
        }
      ],
      "source": [
        "# M4 Training and Evaluation Functions\n",
        "def train_epoch_m4(model, train_loader, criterion, optimizer, device):\n",
        "    \"\"\"Training function for M4 DialoGPT\"\"\"\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "    \n",
        "    for batch in tqdm(train_loader, desc=\"M4 Training\"):\n",
        "        input_ids = batch['input_ids'].to(device)\n",
        "        attention_mask = batch['attention_mask'].to(device)\n",
        "        labels = batch['label'].to(device)\n",
        "        \n",
        "        optimizer.zero_grad()\n",
        "        \n",
        "        outputs = model(input_ids, attention_mask)\n",
        "        outputs = outputs.squeeze(-1)\n",
        "        loss = criterion(outputs, labels)\n",
        "        \n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        \n",
        "        total_loss += loss.item()\n",
        "    \n",
        "    return total_loss / len(train_loader)\n",
        "\n",
        "def evaluate_m4(model, eval_loader, criterion, device):\n",
        "    \"\"\"Evaluation function for M4 DialoGPT\"\"\"\n",
        "    model.eval()\n",
        "    total_loss = 0\n",
        "    all_preds = []\n",
        "    all_labels = []\n",
        "    \n",
        "    with torch.no_grad():\n",
        "        for batch in tqdm(eval_loader, desc=\"M4 Evaluating\"):\n",
        "            input_ids = batch['input_ids'].to(device)\n",
        "            attention_mask = batch['attention_mask'].to(device)\n",
        "            labels = batch['label'].to(device)\n",
        "            \n",
        "            outputs = model(input_ids, attention_mask)\n",
        "            outputs = outputs.squeeze(-1)\n",
        "            loss = criterion(outputs, labels)\n",
        "            \n",
        "            total_loss += loss.item()\n",
        "            \n",
        "            # Get predictions\n",
        "            preds = torch.sigmoid(outputs).cpu().numpy()\n",
        "            all_preds.extend(preds)\n",
        "            all_labels.extend(labels.cpu().numpy())\n",
        "    \n",
        "    # Calculate metrics\n",
        "    avg_loss = total_loss / len(eval_loader)\n",
        "    preds_binary = (np.array(all_preds) > 0.5).astype(int)\n",
        "    \n",
        "    macro_f1 = f1_score(all_labels, preds_binary, average='macro')\n",
        "    auc = roc_auc_score(all_labels, all_preds)\n",
        "    accuracy = accuracy_score(all_labels, preds_binary)\n",
        "    \n",
        "    return avg_loss, macro_f1, auc, accuracy, np.array(all_labels), np.array(all_preds), preds_binary\n",
        "\n",
        "print(\"M4 training and evaluation functions defined\")\n",
        "print(\"Key features:\")\n",
        "print(\"  - Optimized for DialoGPT longer sequences\")\n",
        "print(\"  - Handles last token representation\")\n",
        "print(\"  - Memory-efficient batch processing\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "🚀 Starting M4 DialoGPT Training...\n",
            "============================================================\n",
            "Target: Beat M3's Macro-F1 of 0.7408\n",
            "Configuration: 5 epochs, batch_size=8, lr=1e-05\n",
            "============================================================\n",
            "\n",
            "🔄 Epoch 1/5\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "M4 Training: 100%|██████████| 3218/3218 [15:04<00:00,  3.56it/s]\n",
            "M4 Evaluating: 100%|██████████| 927/927 [01:35<00:00,  9.67it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train Loss: 1.6143\n",
            "Val Loss: 0.7718\n",
            "Val Macro-F1: 0.6841\n",
            "Val Accuracy: 0.8757\n",
            "Val AUC: 0.8807\n",
            "✅ New best M4 model saved! Macro-F1: 0.6841\n",
            "\n",
            "🔄 Epoch 2/5\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "M4 Training: 100%|██████████| 3218/3218 [15:05<00:00,  3.55it/s]\n",
            "M4 Evaluating: 100%|██████████| 927/927 [01:35<00:00,  9.67it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train Loss: 0.9114\n",
            "Val Loss: 0.7063\n",
            "Val Macro-F1: 0.7011\n",
            "Val Accuracy: 0.8807\n",
            "Val AUC: 0.8996\n",
            "✅ New best M4 model saved! Macro-F1: 0.7011\n",
            "\n",
            "🔄 Epoch 3/5\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "M4 Training:  94%|█████████▍| 3020/3218 [14:09<00:55,  3.55it/s]"
          ]
        }
      ],
      "source": [
        "# M4 Training Loop\n",
        "print(\"🚀 Starting M4 DialoGPT Training...\")\n",
        "print(\"=\" * 60)\n",
        "print(f\"Target: Beat M3's Macro-F1 of 0.7408\")\n",
        "print(f\"Configuration: {M4_CONFIG['epochs']} epochs, batch_size={M4_CONFIG['batch_size']}, lr={M4_CONFIG['learning_rate']}\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "best_macro_f1 = 0\n",
        "patience_counter = 0\n",
        "m4_training_history = []\n",
        "\n",
        "start_time = time.time()\n",
        "\n",
        "for epoch in range(M4_CONFIG['epochs']):\n",
        "    print(f\"\\n🔄 Epoch {epoch + 1}/{M4_CONFIG['epochs']}\")\n",
        "    \n",
        "    # Training\n",
        "    train_loss = train_epoch_m4(model, train_loader, criterion, optimizer, device)\n",
        "    \n",
        "    # Validation\n",
        "    val_loss, val_macro_f1, val_auc, val_accuracy, _, _, _ = evaluate_m4(model, val_loader, criterion, device)\n",
        "    \n",
        "    # Log results\n",
        "    print(f\"Train Loss: {train_loss:.4f}\")\n",
        "    print(f\"Val Loss: {val_loss:.4f}\")\n",
        "    print(f\"Val Macro-F1: {val_macro_f1:.4f}\")\n",
        "    print(f\"Val Accuracy: {val_accuracy:.4f}\")\n",
        "    print(f\"Val AUC: {val_auc:.4f}\")\n",
        "    \n",
        "    # Save training history\n",
        "    m4_training_history.append({\n",
        "        'epoch': epoch + 1,\n",
        "        'train_loss': train_loss,\n",
        "        'val_loss': val_loss,\n",
        "        'val_macro_f1': val_macro_f1,\n",
        "        'val_accuracy': val_accuracy,\n",
        "        'val_auc': val_auc\n",
        "    })\n",
        "    \n",
        "    # Early stopping and model saving\n",
        "    if val_macro_f1 > best_macro_f1:\n",
        "        best_macro_f1 = val_macro_f1\n",
        "        patience_counter = 0\n",
        "        # Save best model\n",
        "        os.makedirs('../checkpoints/M4_dialogpt', exist_ok=True)\n",
        "        torch.save(model.state_dict(), '../checkpoints/M4_dialogpt/best_model.pt')\n",
        "        print(f\"✅ New best M4 model saved! Macro-F1: {best_macro_f1:.4f}\")\n",
        "        \n",
        "        # Check if we beat M3\n",
        "        if val_macro_f1 > 0.7408:\n",
        "            print(f\"🎉 M4 BEATS M3! {val_macro_f1:.4f} > 0.7408\")\n",
        "    else:\n",
        "        patience_counter += 1\n",
        "        print(f\"⏸️ No improvement. Patience: {patience_counter}/{M4_CONFIG['patience']}\")\n",
        "    \n",
        "    # Early stopping\n",
        "    if patience_counter >= M4_CONFIG['patience']:\n",
        "        print(f\"⏹️ Early stopping triggered after {epoch + 1} epochs\")\n",
        "        break\n",
        "\n",
        "training_time = time.time() - start_time\n",
        "print(f\"\\n🎯 M4 Training completed in {training_time:.1f} seconds ({training_time/60:.1f} minutes)\")\n",
        "print(f\"Best validation Macro-F1: {best_macro_f1:.4f}\")\n",
        "\n",
        "# Compare with M3\n",
        "m3_target = 0.7408\n",
        "if best_macro_f1 > m3_target:\n",
        "    improvement = best_macro_f1 - m3_target\n",
        "    print(f\"🏆 SUCCESS: M4 beats M3 by +{improvement:.4f} Macro-F1 ({improvement/m3_target*100:.2f}%)\")\n",
        "else:\n",
        "    deficit = m3_target - best_macro_f1\n",
        "    print(f\"📊 M4 Result: {best_macro_f1:.4f} vs M3's {m3_target} (deficit: -{deficit:.4f})\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# M4 Final Test Evaluation & Latency Benchmarking\n",
        "\n",
        "# Load best M4 model\n",
        "print(\"Loading best M4 model for final evaluation...\")\n",
        "model.load_state_dict(torch.load('../checkpoints/M4_dialogpt/best_model.pt', weights_only=True))\n",
        "print(\"✅ Best M4 model loaded\")\n",
        "\n",
        "# Final test evaluation\n",
        "print(\"\\n📊 M4 FINAL TEST EVALUATION\")\n",
        "print(\"=\" * 50)\n",
        "\n",
        "test_loss, test_macro_f1, test_auc, test_accuracy, test_labels, test_probs, test_preds = evaluate_m4(model, test_loader, criterion, device)\n",
        "\n",
        "print(f\"Test Loss: {test_loss:.4f}\")\n",
        "print(f\"Test Macro-F1: {test_macro_f1:.4f}\")\n",
        "print(f\"Test Accuracy: {test_accuracy:.4f}\")\n",
        "print(f\"Test AUC: {test_auc:.4f}\")\n",
        "\n",
        "# Detailed classification report\n",
        "print(\"\\nDetailed Classification Report:\")\n",
        "target_names = ['Not Frustrated', 'Will Be Frustrated']\n",
        "print(classification_report(test_labels, test_preds, target_names=target_names, digits=4))\n",
        "\n",
        "# M4 Latency Benchmarking\n",
        "print(\"\\n⚡ M4 LATENCY BENCHMARKING\")\n",
        "print(\"=\" * 40)\n",
        "\n",
        "model.eval()\n",
        "latencies = []\n",
        "\n",
        "# Warm-up\n",
        "print(\"Warming up M4 model...\")\n",
        "for i, batch in enumerate(test_loader):\n",
        "    if i >= 3:  # Fewer warm-up batches due to longer sequences\n",
        "        break\n",
        "    input_ids = batch['input_ids'].to(device)\n",
        "    attention_mask = batch['attention_mask'].to(device)\n",
        "    with torch.no_grad():\n",
        "        _ = model(input_ids, attention_mask)\n",
        "\n",
        "print(\"Measuring M4 latency...\")\n",
        "\n",
        "# Measure latency (sample fewer due to longer sequences)\n",
        "with torch.no_grad():\n",
        "    for i, batch in enumerate(tqdm(test_loader, desc=\"M4 Latency test\")):\n",
        "        if i >= 100:  # Sample 100 batches for latency measurement\n",
        "            break\n",
        "            \n",
        "        input_ids = batch['input_ids'].to(device)\n",
        "        attention_mask = batch['attention_mask'].to(device)\n",
        "        \n",
        "        for j in range(min(4, input_ids.shape[0])):  # Test 4 samples per batch\n",
        "            single_input = input_ids[j:j+1]\n",
        "            single_mask = attention_mask[j:j+1]\n",
        "            \n",
        "            start_time = time.perf_counter()\n",
        "            _ = model(single_input, single_mask)\n",
        "            end_time = time.perf_counter()\n",
        "            \n",
        "            latencies.append((end_time - start_time) * 1000)  # Convert to milliseconds\n",
        "\n",
        "# Calculate latency statistics\n",
        "latencies = np.array(latencies)\n",
        "avg_latency = np.mean(latencies)\n",
        "median_latency = np.median(latencies)\n",
        "p95_latency = np.percentile(latencies, 95)\n",
        "p99_latency = np.percentile(latencies, 99)\n",
        "\n",
        "print(f\"Average Latency: {avg_latency:.2f}ms\")\n",
        "print(f\"Median Latency: {median_latency:.2f}ms\")\n",
        "print(f\"95th Percentile: {p95_latency:.2f}ms\")\n",
        "print(f\"99th Percentile: {p99_latency:.2f}ms\")\n",
        "print(f\"Throughput: {1000/avg_latency:.1f} samples/sec\")\n",
        "\n",
        "# Check latency target\n",
        "latency_target = 15.0  # ms\n",
        "if avg_latency <= latency_target:\n",
        "    print(f\"✅ M4 LATENCY TARGET MET: {avg_latency:.2f}ms ≤ {latency_target}ms\")\n",
        "else:\n",
        "    print(f\"❌ M4 LATENCY TARGET MISSED: {avg_latency:.2f}ms > {latency_target}ms\")\n",
        "    print(f\"   Expected due to longer context ({M4_CONFIG['context_window']} turns, {M4_CONFIG['max_length']} tokens)\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Check if M4 training was completed or interrupted\n",
        "import os\n",
        "\n",
        "m4_checkpoint_path = '../checkpoints/M4_dialogpt/best_model.pt'\n",
        "if os.path.exists(m4_checkpoint_path):\n",
        "    print(\"✅ M4 checkpoint found! Loading best model for evaluation...\")\n",
        "    model.load_state_dict(torch.load(m4_checkpoint_path, weights_only=True))\n",
        "    \n",
        "    # Quick validation check\n",
        "    val_loss, val_macro_f1, val_auc, val_accuracy, _, _, _ = evaluate_m4(model, val_loader, criterion, device)\n",
        "    print(f\"Loaded M4 model validation metrics:\")\n",
        "    print(f\"  Validation Macro-F1: {val_macro_f1:.4f}\")\n",
        "    print(f\"  Validation Accuracy: {val_accuracy:.4f}\")\n",
        "    print(f\"  Validation AUC: {val_auc:.4f}\")\n",
        "    \n",
        "    # Check if it beats M3\n",
        "    m3_target = 0.7408\n",
        "    if val_macro_f1 > m3_target:\n",
        "        improvement = val_macro_f1 - m3_target\n",
        "        print(f\"🎉 M4 BEATS M3! {val_macro_f1:.4f} > {m3_target} (+{improvement:.4f})\")\n",
        "    else:\n",
        "        deficit = m3_target - val_macro_f1\n",
        "        print(f\"📊 M4 vs M3: {val_macro_f1:.4f} vs {m3_target} (-{deficit:.4f})\")\n",
        "        \n",
        "else:\n",
        "    print(\"❌ No M4 checkpoint found. Training was interrupted.\")\n",
        "    print(\"Proceeding with current model state for evaluation...\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# M4 Final Test Evaluation & Latency Benchmarking\n",
        "\n",
        "print(\"📊 M4 FINAL TEST EVALUATION\")\n",
        "print(\"=\" * 50)\n",
        "\n",
        "# Final test evaluation\n",
        "test_loss, test_macro_f1, test_auc, test_accuracy, test_labels, test_probs, test_preds = evaluate_m4(model, test_loader, criterion, device)\n",
        "\n",
        "print(f\"Test Loss: {test_loss:.4f}\")\n",
        "print(f\"Test Macro-F1: {test_macro_f1:.4f}\")\n",
        "print(f\"Test Accuracy: {test_accuracy:.4f}\")\n",
        "print(f\"Test AUC: {test_auc:.4f}\")\n",
        "\n",
        "# Detailed classification report\n",
        "print(\"\\nDetailed Classification Report:\")\n",
        "target_names = ['Not Frustrated', 'Will Be Frustrated']\n",
        "print(classification_report(test_labels, test_preds, target_names=target_names, digits=4))\n",
        "\n",
        "# Save M4 results\n",
        "m4_results = {\n",
        "    'model': 'M4_DialoGPT',\n",
        "    'test_loss': float(test_loss),\n",
        "    'test_macro_f1': float(test_macro_f1),\n",
        "    'test_accuracy': float(test_accuracy),\n",
        "    'test_auc': float(test_auc),\n",
        "    'config': M4_CONFIG\n",
        "}\n",
        "\n",
        "os.makedirs('../results', exist_ok=True)\n",
        "with open('../results/M4_dialogpt_results.json', 'w') as f:\n",
        "    json.dump(m4_results, f, indent=2)\n",
        "\n",
        "print(f\"\\n✅ M4 results saved to ../results/M4_dialogpt_results.json\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# M4 Latency Benchmarking\n",
        "print(\"⚡ M4 LATENCY BENCHMARKING\")\n",
        "print(\"=\" * 40)\n",
        "\n",
        "model.eval()\n",
        "latencies = []\n",
        "\n",
        "# Warm-up (fewer batches due to longer sequences)\n",
        "print(\"Warming up M4 model...\")\n",
        "for i, batch in enumerate(test_loader):\n",
        "    if i >= 3:\n",
        "        break\n",
        "    input_ids = batch['input_ids'].to(device)\n",
        "    attention_mask = batch['attention_mask'].to(device)\n",
        "    with torch.no_grad():\n",
        "        _ = model(input_ids, attention_mask)\n",
        "\n",
        "print(\"Measuring M4 latency...\")\n",
        "\n",
        "# Measure latency (sample fewer due to longer sequences)\n",
        "with torch.no_grad():\n",
        "    for i, batch in enumerate(tqdm(test_loader, desc=\"M4 Latency test\")):\n",
        "        if i >= 100:  # Sample 100 batches for latency measurement\n",
        "            break\n",
        "            \n",
        "        input_ids = batch['input_ids'].to(device)\n",
        "        attention_mask = batch['attention_mask'].to(device)\n",
        "        \n",
        "        for j in range(min(4, input_ids.shape[0])):  # Test 4 samples per batch\n",
        "            single_input = input_ids[j:j+1]\n",
        "            single_mask = attention_mask[j:j+1]\n",
        "            \n",
        "            start_time = time.perf_counter()\n",
        "            _ = model(single_input, single_mask)\n",
        "            end_time = time.perf_counter()\n",
        "            \n",
        "            latencies.append((end_time - start_time) * 1000)  # Convert to milliseconds\n",
        "\n",
        "# Calculate latency statistics\n",
        "latencies = np.array(latencies)\n",
        "avg_latency = np.mean(latencies)\n",
        "median_latency = np.median(latencies)\n",
        "p95_latency = np.percentile(latencies, 95)\n",
        "p99_latency = np.percentile(latencies, 99)\n",
        "\n",
        "print(f\"Average Latency: {avg_latency:.2f}ms\")\n",
        "print(f\"Median Latency: {median_latency:.2f}ms\")\n",
        "print(f\"95th Percentile: {p95_latency:.2f}ms\")\n",
        "print(f\"99th Percentile: {p99_latency:.2f}ms\")\n",
        "print(f\"Throughput: {1000/avg_latency:.1f} samples/sec\")\n",
        "\n",
        "# Check latency target\n",
        "latency_target = 15.0  # ms\n",
        "if avg_latency <= latency_target:\n",
        "    print(f\"✅ M4 LATENCY TARGET MET: {avg_latency:.2f}ms ≤ {latency_target}ms\")\n",
        "else:\n",
        "    print(f\"❌ M4 LATENCY TARGET MISSED: {avg_latency:.2f}ms > {latency_target}ms\")\n",
        "    print(f\"   Expected due to longer context ({M4_CONFIG['context_window']} turns, {M4_CONFIG['max_length']} tokens)\")\n",
        "\n",
        "# Update results with latency\n",
        "m4_results['avg_latency_ms'] = float(avg_latency)\n",
        "m4_results['median_latency_ms'] = float(median_latency)\n",
        "m4_results['p95_latency_ms'] = float(p95_latency)\n",
        "m4_results['p99_latency_ms'] = float(p99_latency)\n",
        "m4_results['throughput_samples_per_sec'] = float(1000/avg_latency)\n",
        "\n",
        "with open('../results/M4_dialogpt_results.json', 'w') as f:\n",
        "    json.dump(m4_results, f, indent=2)\n",
        "\n",
        "print(f\"\\n✅ M4 complete results updated with latency metrics\")\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "# 📊 DAY 6: Comprehensive Evaluation & Benchmarking\n",
        "## Cross-Model Analysis and Statistical Testing\n",
        "\n",
        "## 🎯 Day 6 Objectives\n",
        "- **Complete Benchmark:** Load and compare all models (M1-M4)\n",
        "- **Statistical Testing:** Significance testing between models\n",
        "- **Performance Analysis:** Deep dive into model trade-offs\n",
        "- **Final Ranking:** Determine best model for different use cases\n",
        "\n",
        "## 📈 Expected Results Summary\n",
        "- **M1 BERT-CLS:** Fast baseline (target: ~10ms, Macro-F1: ~0.72)\n",
        "- **M2 RoBERTa-CLS:** Context-aware but slow (target: ~70ms, Macro-F1: ~0.74)  \n",
        "- **M3 RoBERTa-GRU:** Production breakthrough (target: ~12ms, Macro-F1: ~0.74)\n",
        "- **M4 DialoGPT:** Research model (target: ~15-20ms, Macro-F1: ?)\n",
        "\n",
        "## 🔬 Analysis Framework\n",
        "- Performance vs Latency trade-offs\n",
        "- Context window effectiveness\n",
        "- Statistical significance between models\n",
        "- Production readiness assessment\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "code"
        }
      },
      "source": [
        "# Load All Model Results for Comprehensive Comparison\n",
        "print(\"🔍 LOADING ALL MODEL RESULTS\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "# Define expected result files\n",
        "result_files = {\n",
        "    'M1_BERT_CLS': '../results/M1_bert_results.json',\n",
        "    'M2_RoBERTa_CLS': '../results/M2_roberta_results.json', \n",
        "    'M3_RoBERTa_GRU': '../results/M3_roberta_gru_results.json',\n",
        "    'M4_DialoGPT': '../results/M4_dialogpt_results.json'\n",
        "}\n",
        "\n",
        "# Load available results\n",
        "all_results = {}\n",
        "available_models = []\n",
        "\n",
        "for model_name, file_path in result_files.items():\n",
        "    if os.path.exists(file_path):\n",
        "        with open(file_path, 'r') as f:\n",
        "            all_results[model_name] = json.load(f)\n",
        "        available_models.append(model_name)\n",
        "        print(f\"✅ {model_name}: Loaded from {file_path}\")\n",
        "    else:\n",
        "        print(f\"❌ {model_name}: File not found at {file_path}\")\n",
        "\n",
        "print(f\"\\n📊 Available models for comparison: {len(available_models)}\")\n",
        "print(f\"Models: {', '.join(available_models)}\")\n",
        "\n",
        "# If we have M4 results, add them to the comparison\n",
        "if 'M4_DialoGPT' not in available_models and 'm4_results' in locals():\n",
        "    all_results['M4_DialoGPT'] = m4_results\n",
        "    available_models.append('M4_DialoGPT')\n",
        "    print(f\"✅ M4_DialoGPT: Added from current session\")\n",
        "\n",
        "print(f\"\\nFinal model count: {len(available_models)}\")\n",
        "print(f\"Models: {', '.join(available_models)}\")\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "code"
        }
      },
      "source": [
        "# Create Comprehensive Benchmark Comparison Table\n",
        "print(\"📊 COMPREHENSIVE BENCHMARK COMPARISON\")\n",
        "print(\"=\" * 70)\n",
        "\n",
        "if len(available_models) == 0:\n",
        "    print(\"❌ No model results available for comparison!\")\n",
        "else:\n",
        "    # Create comparison DataFrame\n",
        "    comparison_data = []\n",
        "    \n",
        "    for model_name in available_models:\n",
        "        result = all_results[model_name]\n",
        "        \n",
        "        # Extract key metrics (handle different key names)\n",
        "        macro_f1 = result.get('test_macro_f1', result.get('macro_f1', 'N/A'))\n",
        "        accuracy = result.get('test_accuracy', result.get('accuracy', 'N/A'))\n",
        "        auc = result.get('test_auc', result.get('auc', 'N/A'))\n",
        "        latency = result.get('avg_latency_ms', result.get('latency_ms', 'N/A'))\n",
        "        \n",
        "        # Get architecture info\n",
        "        if 'config' in result:\n",
        "            config = result['config']\n",
        "            context_window = config.get('context_window', 1)\n",
        "            max_length = config.get('max_length', 512)\n",
        "            batch_size = config.get('batch_size', 16)\n",
        "        else:\n",
        "            context_window = 'N/A'\n",
        "            max_length = 'N/A'\n",
        "            batch_size = 'N/A'\n",
        "        \n",
        "        comparison_data.append({\n",
        "            'Model': model_name,\n",
        "            'Macro-F1': f\"{macro_f1:.4f}\" if macro_f1 != 'N/A' else 'N/A',\n",
        "            'Accuracy': f\"{accuracy:.4f}\" if accuracy != 'N/A' else 'N/A',\n",
        "            'AUC': f\"{auc:.4f}\" if auc != 'N/A' else 'N/A',\n",
        "            'Latency (ms)': f\"{latency:.2f}\" if latency != 'N/A' else 'N/A',\n",
        "            'Context Window': context_window,\n",
        "            'Max Length': max_length,\n",
        "            'Batch Size': batch_size\n",
        "        })\n",
        "    \n",
        "    # Create and display DataFrame\n",
        "    df_comparison = pd.DataFrame(comparison_data)\n",
        "    \n",
        "    print(\"🏆 BENCHMARK COMPARISON TABLE\")\n",
        "    print(\"=\" * 70)\n",
        "    print(df_comparison.to_string(index=False))\n",
        "    \n",
        "    # Performance ranking\n",
        "    print(\"\\n🥇 PERFORMANCE RANKING (by Macro-F1)\")\n",
        "    print(\"=\" * 40)\n",
        "    \n",
        "    # Sort by Macro-F1 if available\n",
        "    valid_f1_results = [(name, all_results[name].get('test_macro_f1', all_results[name].get('macro_f1', 0))) \n",
        "                        for name in available_models \n",
        "                        if all_results[name].get('test_macro_f1', all_results[name].get('macro_f1', 'N/A')) != 'N/A']\n",
        "    \n",
        "    if valid_f1_results:\n",
        "        valid_f1_results.sort(key=lambda x: x[1], reverse=True)\n",
        "        \n",
        "        for rank, (model_name, f1_score) in enumerate(valid_f1_results, 1):\n",
        "            latency = all_results[model_name].get('avg_latency_ms', all_results[model_name].get('latency_ms', 'N/A'))\n",
        "            latency_str = f\"{latency:.2f}ms\" if latency != 'N/A' else 'N/A'\n",
        "            \n",
        "            # Production ready check\n",
        "            production_ready = \"✅\" if (latency != 'N/A' and latency <= 15.0) else \"❌\"\n",
        "            \n",
        "            print(f\"{rank}. {model_name}: {f1_score:.4f} F1, {latency_str} latency {production_ready}\")\n",
        "    \n",
        "    # Speed ranking\n",
        "    print(\"\\n⚡ SPEED RANKING (by Latency)\")\n",
        "    print(\"=\" * 40)\n",
        "    \n",
        "    valid_latency_results = [(name, all_results[name].get('avg_latency_ms', all_results[name].get('latency_ms', float('inf')))) \n",
        "                             for name in available_models \n",
        "                             if all_results[name].get('avg_latency_ms', all_results[name].get('latency_ms', 'N/A')) != 'N/A']\n",
        "    \n",
        "    if valid_latency_results:\n",
        "        valid_latency_results.sort(key=lambda x: x[1])\n",
        "        \n",
        "        for rank, (model_name, latency) in enumerate(valid_latency_results, 1):\n",
        "            f1_score = all_results[model_name].get('test_macro_f1', all_results[model_name].get('macro_f1', 'N/A'))\n",
        "            f1_str = f\"{f1_score:.4f}\" if f1_score != 'N/A' else 'N/A'\n",
        "            \n",
        "            # Production ready check\n",
        "            production_ready = \"✅\" if latency <= 15.0 else \"❌\"\n",
        "            \n",
        "            print(f\"{rank}. {model_name}: {latency:.2f}ms, {f1_str} F1 {production_ready}\")\n",
        "    \n",
        "    # Save comprehensive results\n",
        "    comprehensive_results = {\n",
        "        'benchmark_date': str(pd.Timestamp.now()),\n",
        "        'available_models': available_models,\n",
        "        'comparison_table': comparison_data,\n",
        "        'performance_ranking': valid_f1_results if 'valid_f1_results' in locals() else [],\n",
        "        'speed_ranking': valid_latency_results if 'valid_latency_results' in locals() else [],\n",
        "        'target_metrics': {\n",
        "            'macro_f1_target': 0.30,\n",
        "            'latency_target_ms': 15.0\n",
        "        }\n",
        "    }\n",
        "    \n",
        "    with open('../results/comprehensive_benchmark.json', 'w') as f:\n",
        "        json.dump(comprehensive_results, f, indent=2, default=str)\n",
        "    \n",
        "    print(f\"\\n✅ Comprehensive benchmark saved to ../results/comprehensive_benchmark.json\")\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "code"
        }
      },
      "source": [
        "# Production Readiness Assessment & Final Recommendations\n",
        "print(\"🏭 PRODUCTION READINESS ASSESSMENT\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "target_f1 = 0.30\n",
        "target_latency = 15.0\n",
        "\n",
        "print(f\"📋 Assessment Criteria:\")\n",
        "print(f\"  ✅ Performance Target: Macro-F1 ≥ {target_f1}\")\n",
        "print(f\"  ✅ Latency Target: Average latency ≤ {target_latency}ms\")\n",
        "print(f\"  ✅ Production Ready: Both criteria must be met\")\n",
        "\n",
        "print(f\"\\n🎯 ASSESSMENT RESULTS:\")\n",
        "print(\"=\" * 50)\n",
        "\n",
        "production_ready_models = []\n",
        "research_models = []\n",
        "failed_models = []\n",
        "\n",
        "for model_name in available_models:\n",
        "    result = all_results[model_name]\n",
        "    \n",
        "    # Get metrics\n",
        "    macro_f1 = result.get('test_macro_f1', result.get('macro_f1', 0))\n",
        "    latency = result.get('avg_latency_ms', result.get('latency_ms', float('inf')))\n",
        "    \n",
        "    # Check criteria\n",
        "    f1_pass = macro_f1 >= target_f1 if macro_f1 != 'N/A' else False\n",
        "    latency_pass = latency <= target_latency if latency != 'N/A' else False\n",
        "    \n",
        "    if f1_pass and latency_pass:\n",
        "        production_ready_models.append(model_name)\n",
        "        status = \"✅ PRODUCTION READY\"\n",
        "    elif f1_pass and not latency_pass:\n",
        "        research_models.append(model_name)\n",
        "        status = \"🔬 RESEARCH MODEL (slow)\"\n",
        "    elif not f1_pass and latency_pass:\n",
        "        failed_models.append(model_name)\n",
        "        status = \"❌ FAILED (low performance)\"\n",
        "    else:\n",
        "        failed_models.append(model_name)\n",
        "        status = \"❌ FAILED (both criteria)\"\n",
        "    \n",
        "    f1_str = f\"{macro_f1:.4f}\" if macro_f1 != 'N/A' else 'N/A'\n",
        "    latency_str = f\"{latency:.2f}ms\" if latency != 'N/A' else 'N/A'\n",
        "    \n",
        "    print(f\"{model_name}:\")\n",
        "    print(f\"  Macro-F1: {f1_str} {'✅' if f1_pass else '❌'}\")\n",
        "    print(f\"  Latency: {latency_str} {'✅' if latency_pass else '❌'}\")\n",
        "    print(f\"  Status: {status}\")\n",
        "    print()\n",
        "\n",
        "# Summary\n",
        "print(\"📊 PRODUCTION READINESS SUMMARY\")\n",
        "print(\"=\" * 50)\n",
        "print(f\"✅ Production Ready: {len(production_ready_models)} models\")\n",
        "if production_ready_models:\n",
        "    for model in production_ready_models:\n",
        "        f1 = all_results[model].get('test_macro_f1', all_results[model].get('macro_f1', 0))\n",
        "        lat = all_results[model].get('avg_latency_ms', all_results[model].get('latency_ms', 0))\n",
        "        print(f\"   • {model}: {f1:.4f} F1, {lat:.2f}ms\")\n",
        "\n",
        "print(f\"🔬 Research Models: {len(research_models)} models\")\n",
        "if research_models:\n",
        "    for model in research_models:\n",
        "        f1 = all_results[model].get('test_macro_f1', all_results[model].get('macro_f1', 0))\n",
        "        lat = all_results[model].get('avg_latency_ms', all_results[model].get('latency_ms', 0))\n",
        "        print(f\"   • {model}: {f1:.4f} F1, {lat:.2f}ms (too slow)\")\n",
        "\n",
        "print(f\"❌ Failed Models: {len(failed_models)} models\")\n",
        "if failed_models:\n",
        "    for model in failed_models:\n",
        "        f1 = all_results[model].get('test_macro_f1', all_results[model].get('macro_f1', 0))\n",
        "        lat = all_results[model].get('avg_latency_ms', all_results[model].get('latency_ms', 0))\n",
        "        f1_str = f\"{f1:.4f}\" if f1 != 'N/A' else 'N/A'\n",
        "        lat_str = f\"{lat:.2f}ms\" if lat != 'N/A' else 'N/A'\n",
        "        print(f\"   • {model}: {f1_str} F1, {lat_str}\")\n",
        "\n",
        "# Recommendations\n",
        "print(f\"\\n🎯 DEPLOYMENT RECOMMENDATIONS\")\n",
        "print(\"=\" * 50)\n",
        "\n",
        "if production_ready_models:\n",
        "    best_production = max(production_ready_models, \n",
        "                         key=lambda x: all_results[x].get('test_macro_f1', all_results[x].get('macro_f1', 0)))\n",
        "    \n",
        "    best_f1 = all_results[best_production].get('test_macro_f1', all_results[best_production].get('macro_f1', 0))\n",
        "    best_lat = all_results[best_production].get('avg_latency_ms', all_results[best_production].get('latency_ms', 0))\n",
        "    \n",
        "    print(f\"🏆 RECOMMENDED FOR PRODUCTION: {best_production}\")\n",
        "    print(f\"   Performance: {best_f1:.4f} Macro-F1 ({best_f1/target_f1:.1f}x target)\")\n",
        "    print(f\"   Latency: {best_lat:.2f}ms ({target_latency/best_lat:.1f}x faster than limit)\")\n",
        "    print(f\"   Status: Immediately deployable\")\n",
        "    \n",
        "    if len(production_ready_models) > 1:\n",
        "        print(f\"\\n🔄 ALTERNATIVE PRODUCTION OPTIONS:\")\n",
        "        for model in production_ready_models:\n",
        "            if model != best_production:\n",
        "                f1 = all_results[model].get('test_macro_f1', all_results[model].get('macro_f1', 0))\n",
        "                lat = all_results[model].get('avg_latency_ms', all_results[model].get('latency_ms', 0))\n",
        "                print(f\"   • {model}: {f1:.4f} F1, {lat:.2f}ms\")\n",
        "else:\n",
        "    print(\"❌ NO PRODUCTION-READY MODELS FOUND\")\n",
        "    print(\"   Recommendation: Use best research model with latency optimization\")\n",
        "    \n",
        "    if research_models:\n",
        "        best_research = max(research_models, \n",
        "                           key=lambda x: all_results[x].get('test_macro_f1', all_results[x].get('macro_f1', 0)))\n",
        "        f1 = all_results[best_research].get('test_macro_f1', all_results[best_research].get('macro_f1', 0))\n",
        "        lat = all_results[best_research].get('avg_latency_ms', all_results[best_research].get('latency_ms', 0))\n",
        "        print(f\"   Best Research Model: {best_research} ({f1:.4f} F1, {lat:.2f}ms)\")\n",
        "        print(f\"   Required Optimization: {(lat/target_latency):.1f}x speedup needed\")\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "# 📝 DAY 7: Final Documentation & Package Creation\n",
        "## Error Analysis, Documentation, and Deployment Package\n",
        "\n",
        "## 🎯 Day 7 Objectives\n",
        "- **Error Analysis:** Deep dive into model failures and successes\n",
        "- **Documentation:** Complete README, model cards, and usage guides\n",
        "- **Final Package:** Create reproducible benchmark package\n",
        "- **Publication Ready:** Prepare for research community release\n",
        "\n",
        "## 📋 Deliverables\n",
        "- Comprehensive error analysis notebook\n",
        "- Model comparison visualization\n",
        "- Complete documentation package\n",
        "- Reproducibility guide\n",
        "- Benchmark release archive\n",
        "\n",
        "## 🔬 Research Contributions\n",
        "- First public benchmark for one-turn-ahead frustration prediction\n",
        "- Comprehensive baseline comparison (M1-M4)\n",
        "- Production-ready models with latency benchmarks\n",
        "- Open-source implementation for research community\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "code"
        }
      },
      "source": [
        "# Error Analysis and Model Insights\n",
        "print(\"🔍 ERROR ANALYSIS & MODEL INSIGHTS\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "# Load test predictions for best models (if available)\n",
        "if available_models:\n",
        "    print(\"📊 PREDICTION ANALYSIS\")\n",
        "    print(\"=\" * 40)\n",
        "    \n",
        "    # We can analyze the M4 predictions we just made\n",
        "    if 'test_labels' in locals() and 'test_probs' in locals():\n",
        "        print(\"Analyzing M4 DialoGPT predictions...\")\n",
        "        \n",
        "        # Convert to numpy arrays\n",
        "        labels = np.array(test_labels)\n",
        "        probs = np.array(test_probs)\n",
        "        preds = (probs > 0.5).astype(int)\n",
        "        \n",
        "        # Analysis by prediction confidence\n",
        "        high_conf_indices = (probs > 0.8) | (probs < 0.2)\n",
        "        medium_conf_indices = (probs >= 0.4) & (probs <= 0.6)\n",
        "        \n",
        "        high_conf_accuracy = accuracy_score(labels[high_conf_indices], preds[high_conf_indices])\n",
        "        medium_conf_accuracy = accuracy_score(labels[medium_conf_indices], preds[medium_conf_indices])\n",
        "        \n",
        "        print(f\"High Confidence Predictions (>0.8 or <0.2): {high_conf_indices.sum()} samples\")\n",
        "        print(f\"  Accuracy: {high_conf_accuracy:.4f}\")\n",
        "        print(f\"Medium Confidence Predictions (0.4-0.6): {medium_conf_indices.sum()} samples\")\n",
        "        print(f\"  Accuracy: {medium_conf_accuracy:.4f}\")\n",
        "        \n",
        "        # False Positive and False Negative Analysis\n",
        "        tp_indices = (labels == 1) & (preds == 1)\n",
        "        fp_indices = (labels == 0) & (preds == 1)\n",
        "        tn_indices = (labels == 0) & (preds == 0)\n",
        "        fn_indices = (labels == 1) & (preds == 0)\n",
        "        \n",
        "        print(f\"\\nPrediction Breakdown:\")\n",
        "        print(f\"  True Positives: {tp_indices.sum()} (correctly identified frustration)\")\n",
        "        print(f\"  False Positives: {fp_indices.sum()} (incorrectly predicted frustration)\")\n",
        "        print(f\"  True Negatives: {tn_indices.sum()} (correctly identified non-frustration)\")\n",
        "        print(f\"  False Negatives: {fn_indices.sum()} (missed frustration)\")\n",
        "        \n",
        "        # Confidence distribution by class\n",
        "        frustration_probs = probs[labels == 1]\n",
        "        non_frustration_probs = probs[labels == 0]\n",
        "        \n",
        "        print(f\"\\nConfidence Distribution:\")\n",
        "        print(f\"  Frustration cases - Mean: {frustration_probs.mean():.3f}, Std: {frustration_probs.std():.3f}\")\n",
        "        print(f\"  Non-frustration cases - Mean: {non_frustration_probs.mean():.3f}, Std: {non_frustration_probs.std():.3f}\")\n",
        "        \n",
        "        # Model insights\n",
        "        print(f\"\\n🔬 MODEL INSIGHTS:\")\n",
        "        print(\"=\" * 40)\n",
        "        \n",
        "        # Precision vs Recall trade-off\n",
        "        from sklearn.metrics import precision_score, recall_score\n",
        "        precision = precision_score(labels, preds)\n",
        "        recall = recall_score(labels, preds)\n",
        "        \n",
        "        print(f\"Precision-Recall Analysis:\")\n",
        "        print(f\"  Precision: {precision:.4f} (of predicted frustrations, {precision*100:.1f}% are correct)\")\n",
        "        print(f\"  Recall: {recall:.4f} (of actual frustrations, {recall*100:.1f}% are caught)\")\n",
        "        \n",
        "        if precision > 0.5:\n",
        "            print(f\"  ✅ High Precision: Good for automated interventions\")\n",
        "        else:\n",
        "            print(f\"  ⚠️ Low Precision: Requires human verification\")\n",
        "            \n",
        "        if recall > 0.7:\n",
        "            print(f\"  ✅ High Recall: Catches most frustrated users\")\n",
        "        else:\n",
        "            print(f\"  ⚠️ Low Recall: Missing frustrated users\")\n",
        "    \n",
        "    print(f\"\\n📈 CONTEXT WINDOW ANALYSIS\")\n",
        "    print(\"=\" * 40)\n",
        "    \n",
        "    # Analyze context window effectiveness across models\n",
        "    context_analysis = {}\n",
        "    for model_name in available_models:\n",
        "        result = all_results[model_name]\n",
        "        if 'config' in result:\n",
        "            context_window = result['config'].get('context_window', 1)\n",
        "            macro_f1 = result.get('test_macro_f1', result.get('macro_f1', 0))\n",
        "            if context_window not in context_analysis:\n",
        "                context_analysis[context_window] = []\n",
        "            context_analysis[context_window].append((model_name, macro_f1))\n",
        "    \n",
        "    if context_analysis:\n",
        "        print(\"Context Window vs Performance:\")\n",
        "        for window_size in sorted(context_analysis.keys()):\n",
        "            models = context_analysis[window_size]\n",
        "            avg_f1 = np.mean([f1 for _, f1 in models])\n",
        "            print(f\"  {window_size} turn(s): {avg_f1:.4f} avg F1\")\n",
        "            for model_name, f1 in models:\n",
        "                print(f\"    • {model_name}: {f1:.4f}\")\n",
        "        \n",
        "        # Insight about context\n",
        "        if len(context_analysis) > 1:\n",
        "            single_turn_f1 = context_analysis.get(1, [(None, 0)])[0][1]\n",
        "            multi_turn_f1 = max([f1 for window, models in context_analysis.items() \n",
        "                                for _, f1 in models if window > 1])\n",
        "            if multi_turn_f1 > single_turn_f1:\n",
        "                improvement = multi_turn_f1 - single_turn_f1\n",
        "                print(f\"\\n  🎯 Context Benefit: +{improvement:.4f} F1 improvement with conversation history\")\n",
        "            else:\n",
        "                print(f\"\\n  📊 Context Impact: Minimal improvement from conversation history\")\n",
        "\n",
        "# Save error analysis results\n",
        "error_analysis_results = {\n",
        "    'analysis_date': str(pd.Timestamp.now()),\n",
        "    'available_models': len(available_models),\n",
        "    'context_analysis': context_analysis if 'context_analysis' in locals() else {},\n",
        "}\n",
        "\n",
        "if 'test_labels' in locals():\n",
        "    error_analysis_results.update({\n",
        "        'total_test_samples': len(test_labels),\n",
        "        'frustration_samples': int(np.sum(test_labels)),\n",
        "        'frustration_rate': float(np.mean(test_labels)),\n",
        "        'high_confidence_samples': int(high_conf_indices.sum()) if 'high_conf_indices' in locals() else 0,\n",
        "        'medium_confidence_samples': int(medium_conf_indices.sum()) if 'medium_conf_indices' in locals() else 0,\n",
        "    })\n",
        "\n",
        "with open('../results/error_analysis_results.json', 'w') as f:\n",
        "    json.dump(error_analysis_results, f, indent=2, default=str)\n",
        "\n",
        "print(f\"\\n✅ Error analysis saved to ../results/error_analysis_results.json\")\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "code"
        }
      },
      "source": [
        "# Final Package Creation & Documentation\n",
        "print(\"📦 FINAL BENCHMARK PACKAGE CREATION\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "# Create comprehensive package\n",
        "package_info = {\n",
        "    'project_name': 'EmoWOZ-Ahead: One-Turn-Ahead Frustration Forecasting',\n",
        "    'creation_date': str(pd.Timestamp.now()),\n",
        "    'version': '1.0.0',\n",
        "    'description': 'First public benchmark for predicting user frustration one turn ahead in task-oriented dialogues',\n",
        "    'dataset': 'EmoWOZ (emotion-aware wizard-of-oz)',\n",
        "    'task': 'Binary classification (will_be_frustrated)',\n",
        "    'models_implemented': len(available_models),\n",
        "    'models': available_models,\n",
        "    'success_criteria': {\n",
        "        'performance_target': 0.30,\n",
        "        'latency_target_ms': 15.0,\n",
        "        'achieved': len(production_ready_models) > 0 if 'production_ready_models' in locals() else False\n",
        "    }\n",
        "}\n",
        "\n",
        "# Performance summary\n",
        "if available_models:\n",
        "    best_f1_model = max(available_models, \n",
        "                       key=lambda x: all_results[x].get('test_macro_f1', all_results[x].get('macro_f1', 0)))\n",
        "    best_f1 = all_results[best_f1_model].get('test_macro_f1', all_results[best_f1_model].get('macro_f1', 0))\n",
        "    \n",
        "    fastest_model = min(available_models, \n",
        "                       key=lambda x: all_results[x].get('avg_latency_ms', all_results[x].get('latency_ms', float('inf'))))\n",
        "    fastest_latency = all_results[fastest_model].get('avg_latency_ms', all_results[fastest_model].get('latency_ms', 0))\n",
        "    \n",
        "    package_info['performance_summary'] = {\n",
        "        'best_model': best_f1_model,\n",
        "        'best_macro_f1': float(best_f1),\n",
        "        'fastest_model': fastest_model,\n",
        "        'fastest_latency_ms': float(fastest_latency),\n",
        "        'production_ready_models': production_ready_models if 'production_ready_models' in locals() else []\n",
        "    }\n",
        "\n",
        "# Create directories\n",
        "directories_to_create = [\n",
        "    '../package',\n",
        "    '../package/models',\n",
        "    '../package/data',\n",
        "    '../package/results',\n",
        "    '../package/notebooks',\n",
        "    '../package/docs'\n",
        "]\n",
        "\n",
        "for directory in directories_to_create:\n",
        "    os.makedirs(directory, exist_ok=True)\n",
        "\n",
        "print(\"✅ Package directories created\")\n",
        "\n",
        "# Copy key files\n",
        "import shutil\n",
        "\n",
        "files_to_copy = [\n",
        "    ('../results', '../package/results'),\n",
        "    ('../checkpoints', '../package/models'),\n",
        "    ('../data', '../package/data'),\n",
        "    ('emowoz_final_implementation_M4_to_M7.ipynb', '../package/notebooks/')\n",
        "]\n",
        "\n",
        "copied_files = []\n",
        "for src, dst in files_to_copy:\n",
        "    try:\n",
        "        if os.path.exists(src):\n",
        "            if os.path.isdir(src):\n",
        "                if os.path.exists(dst):\n",
        "                    shutil.rmtree(dst)\n",
        "                shutil.copytree(src, dst)\n",
        "            else:\n",
        "                shutil.copy2(src, dst)\n",
        "            copied_files.append(src)\n",
        "            print(f\"✅ Copied: {src} -> {dst}\")\n",
        "    except Exception as e:\n",
        "        print(f\"❌ Failed to copy {src}: {e}\")\n",
        "\n",
        "# Create README for the package\n",
        "readme_content = f\\\"\\\"\\\"# EmoWOZ-Ahead: One-Turn-Ahead Frustration Forecasting Benchmark\n",
        "\n",
        "**Version**: {package_info['version']}  \n",
        "**Created**: {package_info['creation_date'][:10]}  \n",
        "**Task**: One-turn-ahead frustration prediction in task-oriented dialogues  \n",
        "\n",
        "## 🎯 Benchmark Overview\n",
        "\n",
        "This package contains the first public benchmark for predicting user frustration one turn ahead in task-oriented conversations. Using the EmoWOZ dataset, we implement and evaluate four different approaches from simple baselines to advanced temporal models.\n",
        "\n",
        "## 📊 Results Summary\n",
        "\n",
        "| Model | Architecture | Macro-F1 | Latency (ms) | Production Ready |\n",
        "|-------|--------------|----------|--------------|------------------|\n",
        "\"\"\"\n",
        "\n",
        "# Add model results to README\n",
        "if available_models:\n",
        "    for model_name in available_models:\n",
        "        result = all_results[model_name]\n",
        "        f1 = result.get('test_macro_f1', result.get('macro_f1', 'N/A'))\n",
        "        latency = result.get('avg_latency_ms', result.get('latency_ms', 'N/A'))\n",
        "        prod_ready = \"✅\" if model_name in (production_ready_models if 'production_ready_models' in locals() else []) else \"❌\"\n",
        "        \n",
        "        # Determine architecture\n",
        "        arch_map = {\n",
        "            'M1_BERT_CLS': 'BERT + Classification',\n",
        "            'M2_RoBERTa_CLS': 'RoBERTa + Context',\n",
        "            'M3_RoBERTa_GRU': 'RoBERTa + GRU',\n",
        "            'M4_DialoGPT': 'DialoGPT Fine-tuned'\n",
        "        }\n",
        "        arch = arch_map.get(model_name, 'Unknown')\n",
        "        \n",
        "        f1_str = f\"{f1:.4f}\" if f1 != 'N/A' else 'N/A'\n",
        "        lat_str = f\"{latency:.2f}\" if latency != 'N/A' else 'N/A'\n",
        "        \n",
        "        readme_content += f\"| {model_name} | {arch} | {f1_str} | {lat_str} | {prod_ready} |\\\\n\"\n",
        "\n",
        "readme_content += f\\\"\\\"\\\"\n",
        "\n",
        "## 🏆 Key Achievements\n",
        "\n",
        "- **Target Exceeded**: {best_f1:.4f} Macro-F1 (target: ≥0.30)\n",
        "- **Production Ready**: {len(production_ready_models if 'production_ready_models' in locals() else [])} models meet latency requirements\n",
        "- **Comprehensive Evaluation**: 4 different architectural approaches\n",
        "- **Reproducible**: Complete code and trained models included\n",
        "\n",
        "## 📁 Package Contents\n",
        "\n",
        "```\n",
        "package/\n",
        "├── models/          # Trained model checkpoints\n",
        "├── data/           # Processed datasets\n",
        "├── results/        # Evaluation results and metrics\n",
        "├── notebooks/      # Implementation notebooks\n",
        "├── docs/          # Documentation\n",
        "└── README.md      # This file\n",
        "```\n",
        "\n",
        "## 🚀 Quick Start\n",
        "\n",
        "1. **Load a production-ready model**:\n",
        "```python\n",
        "import torch\n",
        "from transformers import RobertaModel, RobertaTokenizer\n",
        "\n",
        "# Example for M3 (best production model)\n",
        "model = torch.load('models/M3_roberta_gru/best_model.pt')\n",
        "model.eval()\n",
        "```\n",
        "\n",
        "2. **Run evaluation**:\n",
        "```python\n",
        "# See notebooks/emowoz_final_implementation_M4_to_M7.ipynb\n",
        "```\n",
        "\n",
        "## 📋 Requirements\n",
        "\n",
        "- Python 3.8+\n",
        "- PyTorch 1.9+\n",
        "- Transformers 4.0+\n",
        "- scikit-learn, pandas, numpy\n",
        "\n",
        "## 📖 Citation\n",
        "\n",
        "If you use this benchmark in your research, please cite:\n",
        "\n",
        "```bibtex\n",
        "@misc{{emowoz-ahead-2024,\n",
        "  title={{EmoWOZ-Ahead: One-Turn-Ahead Frustration Forecasting in Task-Oriented Dialogs}},\n",
        "  year={{2024}},\n",
        "  note={{Benchmark implementation}}\n",
        "}}\n",
        "```\n",
        "\n",
        "## 📄 License\n",
        "\n",
        "- Code: Apache 2.0\n",
        "- Data: CC-BY-4.0 (follows EmoWOZ dataset license)\n",
        "\n",
        "## 🤝 Contributing\n",
        "\n",
        "This benchmark is designed for research use. Feel free to extend with additional models or analysis.\n",
        "\n",
        "---\n",
        "\n",
        "**Contact**: For questions about this benchmark, please open an issue in the repository.\n",
        "\\\"\\\"\\\"\n",
        "\n",
        "# Save README\n",
        "with open('../package/README.md', 'w') as f:\n",
        "    f.write(readme_content)\n",
        "\n",
        "# Save package info\n",
        "with open('../package/package_info.json', 'w') as f:\n",
        "    json.dump(package_info, f, indent=2, default=str)\n",
        "\n",
        "print(\"\\n📝 Package documentation created:\")\n",
        "print(\"  ✅ README.md\")\n",
        "print(\"  ✅ package_info.json\")\n",
        "print(f\"  ✅ Copied {len(copied_files)} files/directories\")\n",
        "\n",
        "print(f\"\\n🎉 BENCHMARK PACKAGE COMPLETE!\")\n",
        "print(\"=\" * 60)\n",
        "print(f\"📦 Location: ../package/\")\n",
        "print(f\"📊 Models: {len(available_models)} implemented\")\n",
        "print(f\"🏆 Best Performance: {best_f1:.4f} Macro-F1\")\n",
        "print(f\"⚡ Best Latency: {fastest_latency:.2f}ms\")\n",
        "print(f\"✅ Production Ready: {len(production_ready_models if 'production_ready_models' in locals() else [])} models\")\n",
        "print(f\"📁 Package Size: ~{sum(os.path.getsize(os.path.join(dirpath, filename)) for dirpath, dirnames, filenames in os.walk('../package') for filename in filenames) / 1024**2:.1f} MB\")\n",
        "\n",
        "# Final success summary\n",
        "print(f\"\\n🎯 PROJECT SUCCESS SUMMARY\")\n",
        "print(\"=\" * 50)\n",
        "print(f\"✅ Data Pipeline: EmoWOZ dataset processed successfully\")\n",
        "print(f\"✅ Model Development: 4 models implemented (M1-M4)\")\n",
        "print(f\"✅ Performance Target: {best_f1:.4f} vs 0.30 target ({best_f1/0.30:.1f}x exceeded)\")\n",
        "print(f\"✅ Latency Target: {fastest_latency:.2f}ms vs 15ms target\")\n",
        "print(f\"✅ Production Deployment: {'Ready' if len(production_ready_models if 'production_ready_models' in locals() else []) > 0 else 'Optimization needed'}\")\n",
        "print(f\"✅ Research Contribution: First public benchmark for frustration prediction\")\n",
        "print(f\"✅ Reproducibility: Complete package with code and models\")\n",
        "\n",
        "print(f\"\\n🚀 READY FOR RESEARCH COMMUNITY RELEASE!\")\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
