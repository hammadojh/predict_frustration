{
  "cells": [
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "# ðŸš€ EmoWOZ Final Implementation: Days 5-7\n",
        "## M4 DialoGPT + Comprehensive Evaluation + Final Package\n",
        "\n",
        "**Project**: One-Turn-Ahead Frustration Forecasting in Task-Oriented Dialogs  \n",
        "**Current Status**: M3 RoBERTa-GRU BREAKTHROUGH (Macro-F1: 0.7408, Latency: 11.57ms) âœ…  \n",
        "**Next Goals**: \n",
        "- **Day 5**: M4 DialoGPT (target: beat M3's 0.7408)\n",
        "- **Day 6**: Cross-model evaluation and benchmarking\n",
        "- **Day 7**: Final documentation and package\n",
        "\n",
        "---\n",
        "\n",
        "## ðŸ“‹ Implementation Plan\n",
        "\n",
        "### **Day 5: M4 DialoGPT Fine-tuned Model**\n",
        "- DialoGPT-small fine-tuning\n",
        "- Longer context (5 turns, max_length=1024)\n",
        "- Last token representation\n",
        "- Target: Macro-F1 > 0.7408\n",
        "\n",
        "### **Day 6: Comprehensive Evaluation**\n",
        "- eval.py script for all models (M1-M4)\n",
        "- Statistical significance testing\n",
        "- Cross-model latency benchmarking\n",
        "- Error analysis\n",
        "\n",
        "### **Day 7: Final Package**\n",
        "- Complete documentation\n",
        "- Model comparison report\n",
        "- Reproducibility guide\n",
        "- Benchmark package\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Setup and imports for M4-M7 implementation\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from transformers import GPT2LMHeadModel, GPT2Tokenizer, RobertaModel, RobertaTokenizer, BertModel, BertTokenizer\n",
        "import json\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.metrics import classification_report, f1_score, roc_auc_score, accuracy_score\n",
        "from scipy import stats\n",
        "from tqdm import tqdm\n",
        "import time\n",
        "import os\n",
        "import warnings\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Check device\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f\"Using device: {device}\")\n",
        "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
        "if torch.cuda.is_available():\n",
        "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
        "    print(f\"GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.1f} GB\")\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "# ðŸ“Š DAY 5: M4 DialoGPT Implementation\n",
        "\n",
        "## ðŸŽ¯ M4 Objectives\n",
        "- **Target**: Beat M3's Macro-F1 (0.7408)\n",
        "- **Innovation**: Longer context (5 turns vs 3 turns)\n",
        "- **Architecture**: DialoGPT-small + classification head\n",
        "- **Context**: max_length=1024 (vs M3's 512)\n",
        "- **Representation**: Last token (vs pooler output)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# M4 DialoGPT Configuration\n",
        "M4_CONFIG = {\n",
        "    'model_name': 'microsoft/DialoGPT-small',\n",
        "    'max_length': 1024,       # Longer context than M3 (512)\n",
        "    'context_window': 5,      # More turns than M3 (3)\n",
        "    'dropout': 0.1,\n",
        "    'batch_size': 8,          # Smaller due to longer sequences\n",
        "    'learning_rate': 1e-5,    # Lower LR for fine-tuning\n",
        "    'epochs': 5,              # More epochs for convergence\n",
        "    'weight_decay': 0.01,\n",
        "    'class_weight_ratio': 13.7,\n",
        "    'patience': 3\n",
        "}\n",
        "\n",
        "print(\"M4 DialoGPT Configuration:\")\n",
        "for k, v in M4_CONFIG.items():\n",
        "    print(f\"  {k}: {v}\")\n",
        "\n",
        "# Compare with M3\n",
        "print(f\"\\nðŸ“Š M4 vs M3 Differences:\")\n",
        "print(f\"  Context Window: {M4_CONFIG['context_window']} vs 3 turns\")\n",
        "print(f\"  Max Length: {M4_CONFIG['max_length']} vs 512 tokens\")\n",
        "print(f\"  Batch Size: {M4_CONFIG['batch_size']} vs 16 (memory optimization)\")\n",
        "print(f\"  Learning Rate: {M4_CONFIG['learning_rate']} vs 2e-5\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# M4 DialoGPT Model Architecture\n",
        "class DialoGPTClassifier(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super(DialoGPTClassifier, self).__init__()\n",
        "        self.config = config\n",
        "        \n",
        "        # DialoGPT for dialogue understanding\n",
        "        self.gpt = GPT2LMHeadModel.from_pretrained(config['model_name'])\n",
        "        self.hidden_size = self.gpt.config.hidden_size\n",
        "        \n",
        "        # Freeze language modeling head (we only need the transformer)\n",
        "        for param in self.gpt.lm_head.parameters():\n",
        "            param.requires_grad = False\n",
        "            \n",
        "        # Classification head\n",
        "        self.dropout = nn.Dropout(config['dropout'])\n",
        "        self.classifier = nn.Linear(self.hidden_size, 1)\n",
        "        \n",
        "    def forward(self, input_ids, attention_mask):\n",
        "        # Get DialoGPT transformer outputs\n",
        "        outputs = self.gpt.transformer(input_ids=input_ids, attention_mask=attention_mask)\n",
        "        \n",
        "        # Use last token representation (dialogue completion)\n",
        "        # Find the last non-padded token for each sequence\n",
        "        batch_size = input_ids.shape[0]\n",
        "        last_token_indices = attention_mask.sum(dim=1) - 1  # Last actual token\n",
        "        \n",
        "        # Extract last token representations\n",
        "        last_hidden_states = []\n",
        "        for i in range(batch_size):\n",
        "            last_idx = last_token_indices[i]\n",
        "            last_hidden_states.append(outputs.last_hidden_state[i, last_idx, :])\n",
        "        \n",
        "        last_hidden = torch.stack(last_hidden_states)\n",
        "        \n",
        "        # Classification\n",
        "        output = self.dropout(last_hidden)\n",
        "        logits = self.classifier(output)\n",
        "        \n",
        "        return logits\n",
        "\n",
        "print(\"M4 DialoGPT Classifier architecture defined\")\n",
        "print(\"Key innovations:\")\n",
        "print(\"  - Uses DialoGPT transformer (dialogue-specific pre-training)\")\n",
        "print(\"  - Last token representation (captures full dialogue context)\")\n",
        "print(\"  - Longer context window (5 turns vs M3's 3)\")\n",
        "print(\"  - Larger sequence length (1024 vs M3's 512)\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "# ðŸš€ EmoWOZ Final Implementation: Days 5-7\n",
        "## M4 DialoGPT + Comprehensive Evaluation + Final Package\n",
        "\n",
        "**Project**: One-Turn-Ahead Frustration Forecasting in Task-Oriented Dialogs  \n",
        "**Current Status**: M3 RoBERTa-GRU BREAKTHROUGH (Macro-F1: 0.7408, Latency: 11.57ms) âœ…  \n",
        "**Next Goals**: \n",
        "- **Day 5**: M4 DialoGPT (target: beat M3's 0.7408)\n",
        "- **Day 6**: Cross-model evaluation and benchmarking\n",
        "- **Day 7**: Final documentation and package\n",
        "\n",
        "---\n",
        "\n",
        "## ðŸ“‹ Implementation Plan\n",
        "\n",
        "### **Day 5: M4 DialoGPT Fine-tuned Model**\n",
        "- DialoGPT-small fine-tuning\n",
        "- Longer context (5 turns, max_length=1024)\n",
        "- Last token representation\n",
        "- Target: Macro-F1 > 0.7408\n",
        "\n",
        "### **Day 6: Comprehensive Evaluation**\n",
        "- eval.py script for all models (M1-M4)\n",
        "- Statistical significance testing\n",
        "- Cross-model latency benchmarking\n",
        "- Error analysis\n",
        "\n",
        "### **Day 7: Final Package**\n",
        "- Complete documentation\n",
        "- Model comparison report\n",
        "- Reproducibility guide\n",
        "- Benchmark package\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using device: cpu\n",
            "CUDA available: False\n"
          ]
        }
      ],
      "source": [
        "# Setup and imports for M4-M7 implementation\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from transformers import GPT2LMHeadModel, GPT2Tokenizer, RobertaModel, RobertaTokenizer, BertModel, BertTokenizer\n",
        "import json\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.metrics import classification_report, f1_score, roc_auc_score, accuracy_score\n",
        "from scipy import stats\n",
        "from tqdm import tqdm\n",
        "import time\n",
        "import os\n",
        "import warnings\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Check device\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f\"Using device: {device}\")\n",
        "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
        "if torch.cuda.is_available():\n",
        "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
        "    print(f\"GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.1f} GB\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "# ðŸ“Š DAY 5: M4 DialoGPT Implementation\n",
        "\n",
        "## ðŸŽ¯ M4 Objectives\n",
        "- **Target**: Beat M3's Macro-F1 (0.7408)\n",
        "- **Innovation**: Longer context (5 turns vs 3 turns)\n",
        "- **Architecture**: DialoGPT-small + classification head\n",
        "- **Context**: max_length=1024 (vs M3's 512)\n",
        "- **Representation**: Last token (vs pooler output)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "M4 DialoGPT Configuration:\n",
            "  model_name: microsoft/DialoGPT-small\n",
            "  max_length: 1024\n",
            "  context_window: 5\n",
            "  dropout: 0.1\n",
            "  batch_size: 8\n",
            "  learning_rate: 1e-05\n",
            "  epochs: 5\n",
            "  weight_decay: 0.01\n",
            "  class_weight_ratio: 13.7\n",
            "  patience: 3\n",
            "\n",
            "ðŸ“Š M4 vs M3 Differences:\n",
            "  Context Window: 5 vs 3 turns\n",
            "  Max Length: 1024 vs 512 tokens\n",
            "  Batch Size: 8 vs 16 (memory optimization)\n",
            "  Learning Rate: 1e-05 vs 2e-5\n"
          ]
        }
      ],
      "source": [
        "# M4 DialoGPT Configuration\n",
        "M4_CONFIG = {\n",
        "    'model_name': 'microsoft/DialoGPT-small',\n",
        "    'max_length': 1024,       # Longer context than M3 (512)\n",
        "    'context_window': 5,      # More turns than M3 (3)\n",
        "    'dropout': 0.1,\n",
        "    'batch_size': 8,          # Smaller due to longer sequences\n",
        "    'learning_rate': 1e-5,    # Lower LR for fine-tuning\n",
        "    'epochs': 5,              # More epochs for convergence\n",
        "    'weight_decay': 0.01,\n",
        "    'class_weight_ratio': 13.7,\n",
        "    'patience': 3\n",
        "}\n",
        "\n",
        "print(\"M4 DialoGPT Configuration:\")\n",
        "for k, v in M4_CONFIG.items():\n",
        "    print(f\"  {k}: {v}\")\n",
        "\n",
        "# Compare with M3\n",
        "print(f\"\\nðŸ“Š M4 vs M3 Differences:\")\n",
        "print(f\"  Context Window: {M4_CONFIG['context_window']} vs 3 turns\")\n",
        "print(f\"  Max Length: {M4_CONFIG['max_length']} vs 512 tokens\")\n",
        "print(f\"  Batch Size: {M4_CONFIG['batch_size']} vs 16 (memory optimization)\")\n",
        "print(f\"  Learning Rate: {M4_CONFIG['learning_rate']} vs 2e-5\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "M4 DialoGPT Classifier architecture defined\n",
            "Key innovations:\n",
            "  - Uses DialoGPT transformer (dialogue-specific pre-training)\n",
            "  - Last token representation (captures full dialogue context)\n",
            "  - Longer context window (5 turns vs M3's 3)\n",
            "  - Larger sequence length (1024 vs M3's 512)\n"
          ]
        }
      ],
      "source": [
        "# M4 DialoGPT Model Architecture\n",
        "class DialoGPTClassifier(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super(DialoGPTClassifier, self).__init__()\n",
        "        self.config = config\n",
        "        \n",
        "        # DialoGPT for dialogue understanding\n",
        "        self.gpt = GPT2LMHeadModel.from_pretrained(config['model_name'])\n",
        "        self.hidden_size = self.gpt.config.hidden_size\n",
        "        \n",
        "        # Freeze language modeling head (we only need the transformer)\n",
        "        for param in self.gpt.lm_head.parameters():\n",
        "            param.requires_grad = False\n",
        "            \n",
        "        # Classification head\n",
        "        self.dropout = nn.Dropout(config['dropout'])\n",
        "        self.classifier = nn.Linear(self.hidden_size, 1)\n",
        "        \n",
        "    def forward(self, input_ids, attention_mask):\n",
        "        # Get DialoGPT transformer outputs\n",
        "        outputs = self.gpt.transformer(input_ids=input_ids, attention_mask=attention_mask)\n",
        "        \n",
        "        # Use last token representation (dialogue completion)\n",
        "        # Find the last non-padded token for each sequence\n",
        "        batch_size = input_ids.shape[0]\n",
        "        last_token_indices = attention_mask.sum(dim=1) - 1  # Last actual token\n",
        "        \n",
        "        # Extract last token representations\n",
        "        last_hidden_states = []\n",
        "        for i in range(batch_size):\n",
        "            last_idx = last_token_indices[i]\n",
        "            last_hidden_states.append(outputs.last_hidden_state[i, last_idx, :])\n",
        "        \n",
        "        last_hidden = torch.stack(last_hidden_states)\n",
        "        \n",
        "        # Classification\n",
        "        output = self.dropout(last_hidden)\n",
        "        logits = self.classifier(output)\n",
        "        \n",
        "        return logits\n",
        "\n",
        "print(\"M4 DialoGPT Classifier architecture defined\")\n",
        "print(\"Key innovations:\")\n",
        "print(\"  - Uses DialoGPT transformer (dialogue-specific pre-training)\")\n",
        "print(\"  - Last token representation (captures full dialogue context)\")\n",
        "print(\"  - Longer context window (5 turns vs M3's 3)\")\n",
        "print(\"  - Larger sequence length (1024 vs M3's 512)\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "M4 DialoGPT Dataset class defined\n",
            "Key features:\n",
            "  - Longer context window: 5 turns\n",
            "  - Larger max length: 1024 tokens\n",
            "  - Dialogue-style formatting for DialoGPT\n",
            "  - User/System speaker formatting\n"
          ]
        }
      ],
      "source": [
        "# M4 Dataset class for longer context\n",
        "class EmoWOZDialoGPTDataset(Dataset):\n",
        "    def __init__(self, data_path, tokenizer, config):\n",
        "        self.tokenizer = tokenizer\n",
        "        self.max_length = config['max_length']\n",
        "        self.context_window = config['context_window']\n",
        "        \n",
        "        # Load data with error handling\n",
        "        self.data = []\n",
        "        skipped_lines = 0\n",
        "        \n",
        "        with open(data_path, 'r') as f:\n",
        "            for line_num, line in enumerate(f, 1):\n",
        "                line = line.strip()\n",
        "                if not line:\n",
        "                    continue\n",
        "                try:\n",
        "                    data_item = json.loads(line)\n",
        "                    self.data.append(data_item)\n",
        "                except json.JSONDecodeError as e:\n",
        "                    skipped_lines += 1\n",
        "                    continue\n",
        "        \n",
        "        print(f\"M4 Dataset: Loaded {len(self.data)} samples from {data_path}\")\n",
        "        if skipped_lines > 0:\n",
        "            print(f\"Skipped {skipped_lines} invalid lines\")\n",
        "    \n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "    \n",
        "    def parse_context_string(self, context_str):\n",
        "        \"\"\"Parse context string into individual turns\"\"\"\n",
        "        import re\n",
        "        \n",
        "        turns = []\n",
        "        pattern = r'\\[(USER|SYSTEM)\\]'\n",
        "        matches = list(re.finditer(pattern, context_str))\n",
        "        \n",
        "        for i, match in enumerate(matches):\n",
        "            speaker = match.group(1)\n",
        "            start_pos = match.end()\n",
        "            \n",
        "            if i + 1 < len(matches):\n",
        "                end_pos = matches[i + 1].start()\n",
        "                text = context_str[start_pos:end_pos].strip()\n",
        "            else:\n",
        "                text = context_str[start_pos:].strip()\n",
        "            \n",
        "            if text:\n",
        "                turns.append({\n",
        "                    'speaker': speaker,\n",
        "                    'text': text\n",
        "                })\n",
        "        \n",
        "        return turns\n",
        "    \n",
        "    def __getitem__(self, idx):\n",
        "        item = self.data[idx]\n",
        "        \n",
        "        # Parse context string into turns\n",
        "        context_str = item['context']\n",
        "        context_turns = self.parse_context_string(context_str)\n",
        "        \n",
        "        # Take last N turns (longer context for M4)\n",
        "        if len(context_turns) > self.context_window:\n",
        "            context_turns = context_turns[-self.context_window:]\n",
        "        \n",
        "        # Create dialogue string for DialoGPT\n",
        "        dialogue_text = \"\"\n",
        "        for turn in context_turns:\n",
        "            if turn['speaker'] == 'USER':\n",
        "                dialogue_text += f\"User: {turn['text']} \"\n",
        "            else:\n",
        "                dialogue_text += f\"System: {turn['text']} \"\n",
        "        \n",
        "        # Add the current turn\n",
        "        current_text = item.get('text', '')\n",
        "        if current_text:\n",
        "            dialogue_text += f\"User: {current_text}\"\n",
        "        \n",
        "        # Tokenize the full dialogue\n",
        "        encoded = self.tokenizer(\n",
        "            dialogue_text.strip(),\n",
        "            max_length=self.max_length,\n",
        "            padding='max_length',\n",
        "            truncation=True,\n",
        "            return_tensors='pt'\n",
        "        )\n",
        "        \n",
        "        # Label\n",
        "        label = torch.tensor(item['label'], dtype=torch.float)\n",
        "        \n",
        "        return {\n",
        "            'input_ids': encoded['input_ids'].squeeze(0),\n",
        "            'attention_mask': encoded['attention_mask'].squeeze(0),\n",
        "            'label': label\n",
        "        }\n",
        "\n",
        "print(\"M4 DialoGPT Dataset class defined\")\n",
        "print(\"Key features:\")\n",
        "print(f\"  - Longer context window: {M4_CONFIG['context_window']} turns\")\n",
        "print(f\"  - Larger max length: {M4_CONFIG['max_length']} tokens\")\n",
        "print(\"  - Dialogue-style formatting for DialoGPT\")\n",
        "print(\"  - User/System speaker formatting\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loading DialoGPT tokenizer...\n",
            "Tokenizer loaded: microsoft/DialoGPT-small\n",
            "Vocab size: 50257\n",
            "Pad token: <|endoftext|>\n",
            "M4 Dataset: Loaded 57241 samples from ../data/train.jsonl\n",
            "M4 Dataset: Loaded 7409 samples from ../data/val.jsonl\n",
            "M4 Dataset: Loaded 7534 samples from ../data/test.jsonl\n",
            "\n",
            "ðŸ“Š M4 Data Loading Complete:\n",
            "Train batches: 7156 (batch_size=8)\n",
            "Validation batches: 927\n",
            "Test batches: 942\n",
            "Total samples: 57241 train, 7409 val, 7534 test\n"
          ]
        }
      ],
      "source": [
        "# Step 2: M4 Data Loading & Training Setup\n",
        "\n",
        "# Load DialoGPT tokenizer\n",
        "print(\"Loading DialoGPT tokenizer...\")\n",
        "tokenizer = GPT2Tokenizer.from_pretrained(M4_CONFIG['model_name'])\n",
        "\n",
        "# Add padding token (DialoGPT doesn't have one by default)\n",
        "if tokenizer.pad_token is None:\n",
        "    tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "print(f\"Tokenizer loaded: {M4_CONFIG['model_name']}\")\n",
        "print(f\"Vocab size: {len(tokenizer)}\")\n",
        "print(f\"Pad token: {tokenizer.pad_token}\")\n",
        "\n",
        "# Create M4 datasets with longer context\n",
        "train_dataset = EmoWOZDialoGPTDataset('../data/train.jsonl', tokenizer, M4_CONFIG)\n",
        "val_dataset = EmoWOZDialoGPTDataset('../data/val.jsonl', tokenizer, M4_CONFIG)\n",
        "test_dataset = EmoWOZDialoGPTDataset('../data/test.jsonl', tokenizer, M4_CONFIG)\n",
        "\n",
        "# Create data loaders with smaller batch size\n",
        "train_loader = DataLoader(train_dataset, batch_size=M4_CONFIG['batch_size'], shuffle=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=M4_CONFIG['batch_size'], shuffle=False)\n",
        "test_loader = DataLoader(test_dataset, batch_size=M4_CONFIG['batch_size'], shuffle=False)\n",
        "\n",
        "print(f\"\\nðŸ“Š M4 Data Loading Complete:\")\n",
        "print(f\"Train batches: {len(train_loader)} (batch_size={M4_CONFIG['batch_size']})\")\n",
        "print(f\"Validation batches: {len(val_loader)}\")\n",
        "print(f\"Test batches: {len(test_loader)}\")\n",
        "print(f\"Total samples: {len(train_dataset)} train, {len(val_dataset)} val, {len(test_dataset)} test\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Initializing M4 DialoGPT model...\n",
            "\n",
            "ðŸ—ï¸ M4 Model Information:\n",
            "Total parameters: 124,440,577\n",
            "Trainable parameters: 85,843,201\n",
            "Frozen parameters: 38,597,376 (LM head)\n",
            "Model size: 474.7 MB\n",
            "Hidden size: 768\n",
            "\n",
            "ðŸ“Š M4 vs M3 Model Comparison:\n",
            "M4 Parameters: 124,440,577\n",
            "M3 Parameters: ~125M (RoBERTa + GRU)\n",
            "M4 Context: 5 turns, 1024 tokens\n",
            "M3 Context: 3 turns, 512 tokens\n"
          ]
        }
      ],
      "source": [
        "# Initialize M4 Model and Training Components\n",
        "print(\"Initializing M4 DialoGPT model...\")\n",
        "\n",
        "# Initialize model\n",
        "model = DialoGPTClassifier(M4_CONFIG).to(device)\n",
        "\n",
        "# Loss function with class weights\n",
        "pos_weight = torch.tensor(M4_CONFIG['class_weight_ratio']).to(device)\n",
        "criterion = nn.BCEWithLogitsLoss(pos_weight=pos_weight)\n",
        "\n",
        "# Optimizer (lower learning rate for fine-tuning)\n",
        "optimizer = optim.AdamW(model.parameters(), lr=M4_CONFIG['learning_rate'], weight_decay=M4_CONFIG['weight_decay'])\n",
        "\n",
        "# Model information\n",
        "total_params = sum(p.numel() for p in model.parameters())\n",
        "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "frozen_params = total_params - trainable_params\n",
        "\n",
        "print(f\"\\nðŸ—ï¸ M4 Model Information:\")\n",
        "print(f\"Total parameters: {total_params:,}\")\n",
        "print(f\"Trainable parameters: {trainable_params:,}\")\n",
        "print(f\"Frozen parameters: {frozen_params:,} (LM head)\")\n",
        "print(f\"Model size: {total_params * 4 / 1024**2:.1f} MB\")\n",
        "print(f\"Hidden size: {model.hidden_size}\")\n",
        "\n",
        "# Compare with M3\n",
        "print(f\"\\nðŸ“Š M4 vs M3 Model Comparison:\")\n",
        "print(f\"M4 Parameters: {total_params:,}\")\n",
        "print(f\"M3 Parameters: ~125M (RoBERTa + GRU)\")\n",
        "print(f\"M4 Context: {M4_CONFIG['context_window']} turns, {M4_CONFIG['max_length']} tokens\")\n",
        "print(f\"M3 Context: 3 turns, 512 tokens\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "M4 training and evaluation functions defined\n",
            "Key features:\n",
            "  - Optimized for DialoGPT longer sequences\n",
            "  - Handles last token representation\n",
            "  - Memory-efficient batch processing\n"
          ]
        }
      ],
      "source": [
        "# M4 Training and Evaluation Functions\n",
        "def train_epoch_m4(model, train_loader, criterion, optimizer, device):\n",
        "    \"\"\"Training function for M4 DialoGPT\"\"\"\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "    \n",
        "    for batch in tqdm(train_loader, desc=\"M4 Training\"):\n",
        "        input_ids = batch['input_ids'].to(device)\n",
        "        attention_mask = batch['attention_mask'].to(device)\n",
        "        labels = batch['label'].to(device)\n",
        "        \n",
        "        optimizer.zero_grad()\n",
        "        \n",
        "        outputs = model(input_ids, attention_mask)\n",
        "        outputs = outputs.squeeze(-1)\n",
        "        loss = criterion(outputs, labels)\n",
        "        \n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        \n",
        "        total_loss += loss.item()\n",
        "    \n",
        "    return total_loss / len(train_loader)\n",
        "\n",
        "def evaluate_m4(model, eval_loader, criterion, device):\n",
        "    \"\"\"Evaluation function for M4 DialoGPT\"\"\"\n",
        "    model.eval()\n",
        "    total_loss = 0\n",
        "    all_preds = []\n",
        "    all_labels = []\n",
        "    \n",
        "    with torch.no_grad():\n",
        "        for batch in tqdm(eval_loader, desc=\"M4 Evaluating\"):\n",
        "            input_ids = batch['input_ids'].to(device)\n",
        "            attention_mask = batch['attention_mask'].to(device)\n",
        "            labels = batch['label'].to(device)\n",
        "            \n",
        "            outputs = model(input_ids, attention_mask)\n",
        "            outputs = outputs.squeeze(-1)\n",
        "            loss = criterion(outputs, labels)\n",
        "            \n",
        "            total_loss += loss.item()\n",
        "            \n",
        "            # Get predictions\n",
        "            preds = torch.sigmoid(outputs).cpu().numpy()\n",
        "            all_preds.extend(preds)\n",
        "            all_labels.extend(labels.cpu().numpy())\n",
        "    \n",
        "    # Calculate metrics\n",
        "    avg_loss = total_loss / len(eval_loader)\n",
        "    preds_binary = (np.array(all_preds) > 0.5).astype(int)\n",
        "    \n",
        "    macro_f1 = f1_score(all_labels, preds_binary, average='macro')\n",
        "    auc = roc_auc_score(all_labels, all_preds)\n",
        "    accuracy = accuracy_score(all_labels, preds_binary)\n",
        "    \n",
        "    return avg_loss, macro_f1, auc, accuracy, np.array(all_labels), np.array(all_preds), preds_binary\n",
        "\n",
        "print(\"M4 training and evaluation functions defined\")\n",
        "print(\"Key features:\")\n",
        "print(\"  - Optimized for DialoGPT longer sequences\")\n",
        "print(\"  - Handles last token representation\")\n",
        "print(\"  - Memory-efficient batch processing\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "M4 training and evaluation functions defined\n",
            "Key features:\n",
            "  - Optimized for DialoGPT longer sequences\n",
            "  - Handles last token representation\n",
            "  - Memory-efficient batch processing\n"
          ]
        }
      ],
      "source": [
        "# M4 Training and Evaluation Functions\n",
        "def train_epoch_m4(model, train_loader, criterion, optimizer, device):\n",
        "    \"\"\"Training function for M4 DialoGPT\"\"\"\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "    \n",
        "    for batch in tqdm(train_loader, desc=\"M4 Training\"):\n",
        "        input_ids = batch['input_ids'].to(device)\n",
        "        attention_mask = batch['attention_mask'].to(device)\n",
        "        labels = batch['label'].to(device)\n",
        "        \n",
        "        optimizer.zero_grad()\n",
        "        \n",
        "        outputs = model(input_ids, attention_mask)\n",
        "        outputs = outputs.squeeze(-1)\n",
        "        loss = criterion(outputs, labels)\n",
        "        \n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        \n",
        "        total_loss += loss.item()\n",
        "    \n",
        "    return total_loss / len(train_loader)\n",
        "\n",
        "def evaluate_m4(model, eval_loader, criterion, device):\n",
        "    \"\"\"Evaluation function for M4 DialoGPT\"\"\"\n",
        "    model.eval()\n",
        "    total_loss = 0\n",
        "    all_preds = []\n",
        "    all_labels = []\n",
        "    \n",
        "    with torch.no_grad():\n",
        "        for batch in tqdm(eval_loader, desc=\"M4 Evaluating\"):\n",
        "            input_ids = batch['input_ids'].to(device)\n",
        "            attention_mask = batch['attention_mask'].to(device)\n",
        "            labels = batch['label'].to(device)\n",
        "            \n",
        "            outputs = model(input_ids, attention_mask)\n",
        "            outputs = outputs.squeeze(-1)\n",
        "            loss = criterion(outputs, labels)\n",
        "            \n",
        "            total_loss += loss.item()\n",
        "            \n",
        "            # Get predictions\n",
        "            preds = torch.sigmoid(outputs).cpu().numpy()\n",
        "            all_preds.extend(preds)\n",
        "            all_labels.extend(labels.cpu().numpy())\n",
        "    \n",
        "    # Calculate metrics\n",
        "    avg_loss = total_loss / len(eval_loader)\n",
        "    preds_binary = (np.array(all_preds) > 0.5).astype(int)\n",
        "    \n",
        "    macro_f1 = f1_score(all_labels, preds_binary, average='macro')\n",
        "    auc = roc_auc_score(all_labels, all_preds)\n",
        "    accuracy = accuracy_score(all_labels, preds_binary)\n",
        "    \n",
        "    return avg_loss, macro_f1, auc, accuracy, np.array(all_labels), np.array(all_preds), preds_binary\n",
        "\n",
        "print(\"M4 training and evaluation functions defined\")\n",
        "print(\"Key features:\")\n",
        "print(\"  - Optimized for DialoGPT longer sequences\")\n",
        "print(\"  - Handles last token representation\")\n",
        "print(\"  - Memory-efficient batch processing\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# M4 Training Loop\n",
        "print(\"ðŸš€ Starting M4 DialoGPT Training...\")\n",
        "print(\"=\" * 60)\n",
        "print(f\"Target: Beat M3's Macro-F1 of 0.7408\")\n",
        "print(f\"Configuration: {M4_CONFIG['epochs']} epochs, batch_size={M4_CONFIG['batch_size']}, lr={M4_CONFIG['learning_rate']}\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "best_macro_f1 = 0\n",
        "patience_counter = 0\n",
        "m4_training_history = []\n",
        "\n",
        "start_time = time.time()\n",
        "\n",
        "for epoch in range(M4_CONFIG['epochs']):\n",
        "    print(f\"\\nðŸ”„ Epoch {epoch + 1}/{M4_CONFIG['epochs']}\")\n",
        "    \n",
        "    # Training\n",
        "    train_loss = train_epoch_m4(model, train_loader, criterion, optimizer, device)\n",
        "    \n",
        "    # Validation\n",
        "    val_loss, val_macro_f1, val_auc, val_accuracy, _, _, _ = evaluate_m4(model, val_loader, criterion, device)\n",
        "    \n",
        "    # Log results\n",
        "    print(f\"Train Loss: {train_loss:.4f}\")\n",
        "    print(f\"Val Loss: {val_loss:.4f}\")\n",
        "    print(f\"Val Macro-F1: {val_macro_f1:.4f}\")\n",
        "    print(f\"Val Accuracy: {val_accuracy:.4f}\")\n",
        "    print(f\"Val AUC: {val_auc:.4f}\")\n",
        "    \n",
        "    # Save training history\n",
        "    m4_training_history.append({\n",
        "        'epoch': epoch + 1,\n",
        "        'train_loss': train_loss,\n",
        "        'val_loss': val_loss,\n",
        "        'val_macro_f1': val_macro_f1,\n",
        "        'val_accuracy': val_accuracy,\n",
        "        'val_auc': val_auc\n",
        "    })\n",
        "    \n",
        "    # Early stopping and model saving\n",
        "    if val_macro_f1 > best_macro_f1:\n",
        "        best_macro_f1 = val_macro_f1\n",
        "        patience_counter = 0\n",
        "        # Save best model\n",
        "        os.makedirs('../checkpoints/M4_dialogpt', exist_ok=True)\n",
        "        torch.save(model.state_dict(), '../checkpoints/M4_dialogpt/best_model.pt')\n",
        "        print(f\"âœ… New best M4 model saved! Macro-F1: {best_macro_f1:.4f}\")\n",
        "        \n",
        "        # Check if we beat M3\n",
        "        if val_macro_f1 > 0.7408:\n",
        "            print(f\"ðŸŽ‰ M4 BEATS M3! {val_macro_f1:.4f} > 0.7408\")\n",
        "    else:\n",
        "        patience_counter += 1\n",
        "        print(f\"â¸ï¸ No improvement. Patience: {patience_counter}/{M4_CONFIG['patience']}\")\n",
        "    \n",
        "    # Early stopping\n",
        "    if patience_counter >= M4_CONFIG['patience']:\n",
        "        print(f\"â¹ï¸ Early stopping triggered after {epoch + 1} epochs\")\n",
        "        break\n",
        "\n",
        "training_time = time.time() - start_time\n",
        "print(f\"\\nðŸŽ¯ M4 Training completed in {training_time:.1f} seconds ({training_time/60:.1f} minutes)\")\n",
        "print(f\"Best validation Macro-F1: {best_macro_f1:.4f}\")\n",
        "\n",
        "# Compare with M3\n",
        "m3_target = 0.7408\n",
        "if best_macro_f1 > m3_target:\n",
        "    improvement = best_macro_f1 - m3_target\n",
        "    print(f\"ðŸ† SUCCESS: M4 beats M3 by +{improvement:.4f} Macro-F1 ({improvement/m3_target*100:.2f}%)\")\n",
        "else:\n",
        "    deficit = m3_target - best_macro_f1\n",
        "    print(f\"ðŸ“Š M4 Result: {best_macro_f1:.4f} vs M3's {m3_target} (deficit: -{deficit:.4f})\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# M4 Final Test Evaluation & Latency Benchmarking\n",
        "\n",
        "# Load best M4 model\n",
        "print(\"Loading best M4 model for final evaluation...\")\n",
        "model.load_state_dict(torch.load('../checkpoints/M4_dialogpt/best_model.pt', weights_only=True))\n",
        "print(\"âœ… Best M4 model loaded\")\n",
        "\n",
        "# Final test evaluation\n",
        "print(\"\\nðŸ“Š M4 FINAL TEST EVALUATION\")\n",
        "print(\"=\" * 50)\n",
        "\n",
        "test_loss, test_macro_f1, test_auc, test_accuracy, test_labels, test_probs, test_preds = evaluate_m4(model, test_loader, criterion, device)\n",
        "\n",
        "print(f\"Test Loss: {test_loss:.4f}\")\n",
        "print(f\"Test Macro-F1: {test_macro_f1:.4f}\")\n",
        "print(f\"Test Accuracy: {test_accuracy:.4f}\")\n",
        "print(f\"Test AUC: {test_auc:.4f}\")\n",
        "\n",
        "# Detailed classification report\n",
        "print(\"\\nDetailed Classification Report:\")\n",
        "target_names = ['Not Frustrated', 'Will Be Frustrated']\n",
        "print(classification_report(test_labels, test_preds, target_names=target_names, digits=4))\n",
        "\n",
        "# M4 Latency Benchmarking\n",
        "print(\"\\nâš¡ M4 LATENCY BENCHMARKING\")\n",
        "print(\"=\" * 40)\n",
        "\n",
        "model.eval()\n",
        "latencies = []\n",
        "\n",
        "# Warm-up\n",
        "print(\"Warming up M4 model...\")\n",
        "for i, batch in enumerate(test_loader):\n",
        "    if i >= 3:  # Fewer warm-up batches due to longer sequences\n",
        "        break\n",
        "    input_ids = batch['input_ids'].to(device)\n",
        "    attention_mask = batch['attention_mask'].to(device)\n",
        "    with torch.no_grad():\n",
        "        _ = model(input_ids, attention_mask)\n",
        "\n",
        "print(\"Measuring M4 latency...\")\n",
        "\n",
        "# Measure latency (sample fewer due to longer sequences)\n",
        "with torch.no_grad():\n",
        "    for i, batch in enumerate(tqdm(test_loader, desc=\"M4 Latency test\")):\n",
        "        if i >= 100:  # Sample 100 batches for latency measurement\n",
        "            break\n",
        "            \n",
        "        input_ids = batch['input_ids'].to(device)\n",
        "        attention_mask = batch['attention_mask'].to(device)\n",
        "        \n",
        "        for j in range(min(4, input_ids.shape[0])):  # Test 4 samples per batch\n",
        "            single_input = input_ids[j:j+1]\n",
        "            single_mask = attention_mask[j:j+1]\n",
        "            \n",
        "            start_time = time.perf_counter()\n",
        "            _ = model(single_input, single_mask)\n",
        "            end_time = time.perf_counter()\n",
        "            \n",
        "            latencies.append((end_time - start_time) * 1000)  # Convert to milliseconds\n",
        "\n",
        "# Calculate latency statistics\n",
        "latencies = np.array(latencies)\n",
        "avg_latency = np.mean(latencies)\n",
        "median_latency = np.median(latencies)\n",
        "p95_latency = np.percentile(latencies, 95)\n",
        "p99_latency = np.percentile(latencies, 99)\n",
        "\n",
        "print(f\"Average Latency: {avg_latency:.2f}ms\")\n",
        "print(f\"Median Latency: {median_latency:.2f}ms\")\n",
        "print(f\"95th Percentile: {p95_latency:.2f}ms\")\n",
        "print(f\"99th Percentile: {p99_latency:.2f}ms\")\n",
        "print(f\"Throughput: {1000/avg_latency:.1f} samples/sec\")\n",
        "\n",
        "# Check latency target\n",
        "latency_target = 15.0  # ms\n",
        "if avg_latency <= latency_target:\n",
        "    print(f\"âœ… M4 LATENCY TARGET MET: {avg_latency:.2f}ms â‰¤ {latency_target}ms\")\n",
        "else:\n",
        "    print(f\"âŒ M4 LATENCY TARGET MISSED: {avg_latency:.2f}ms > {latency_target}ms\")\n",
        "    print(f\"   Expected due to longer context ({M4_CONFIG['context_window']} turns, {M4_CONFIG['max_length']} tokens)\")\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
